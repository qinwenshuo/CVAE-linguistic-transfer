{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qinwenshuo/CVAE-linguistic-transfer/blob/main/2_machamp_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use T4/V100 GPU\n",
        "\n",
        "# There is always one essay in the ONAC corpus that is causing runtime error. I cann't locate\n",
        "# that file because it changes each time I parse the corpus. Here is the solution:\n",
        "# You can start with any big number of concatenation_batch like 500.\n",
        "# You will encounter error: Invalid input(likely to be caused by GPU out of memory),\n",
        "# just change the concatenation_batch to a small number like 50 and run again(make\n",
        "# sure you archieved the parsed results before run again). You will still encounter\n",
        "# that error, but no worry, you can just repeat the process, change a small number\n",
        "# and run again until there is only one essay left\n",
        "\n",
        "concatenation_batch = 500"
      ],
      "metadata": {
        "id": "73nw_zKkkVQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvTQQKWWk4r0",
        "outputId": "1a191149-71f4-4eed-b963-8d1ef7214048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Cph-I4lWjG",
        "outputId": "ba14ab51-412f-46a9-cfad-3e97fa538aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please choose one language from the list: ['Arabic', 'English', 'Spanish', 'Croatian', 'Czech', 'German', 'Italian', 'Norwegian', 'Portuguese', 'Latvian', 'Icelandic', 'Finnish', 'Chinese', 'Korean']\n",
            "Enter your choice: English\n",
            "Native or Learner?\n",
            "Enter your choice: Learner\n",
            "Please choose one corpus from the list: ['WriCLE', 'ArabCC', 'ICNALE', 'ICLE', 'PELIC', 'TOEFL', 'WriCLE_informal', 'BAWE', 'Gachon', 'CLC']\n",
            "Enter your choice: CLC\n",
            "\n",
            "input_directory: /content/drive/MyDrive/cvae_project/2_conllu_format/English/Learner/CLC.zip\n",
            "output_directory: /content/CLC/\n",
            "archived_output_directory: /content/drive/MyDrive/cvae_project/3_pred_format/English/Learner/CLC_machamp_mbert_1\n",
            "logs_dir: /content/drive/MyDrive/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34/model.tar.gz\n",
            "predict_dir: /content/drive/MyDrive/crosslinguistic_nli/machamp/predict.py\n"
          ]
        }
      ],
      "source": [
        "explanation_dict = {\n",
        "    'Arabic': 'ar',\n",
        "    'English': 'en',\n",
        "    'Spanish': 'es',\n",
        "    'Croatian': 'hr',\n",
        "    'Czech': 'cs',\n",
        "    'German': 'de',\n",
        "    'Italian': 'it',\n",
        "    'Norwegian': 'no',\n",
        "    'Portuguese': 'pt',\n",
        "    'Latvian': 'lv',\n",
        "    'Icelandic': 'is',\n",
        "    'Finnish': 'fi',\n",
        "    'Chinese': 'zh',\n",
        "    'Korean': 'ko'\n",
        "}\n",
        "print(\"Please choose one language from the list:\", list(explanation_dict.keys()))\n",
        "language = input(\"Enter your choice: \")\n",
        "nativeness = input(\"Native or Learner?\\nEnter your choice: \")\n",
        "input_directory = f'/content/drive/MyDrive/cvae_project/2_conllu_format/{language}/{nativeness}/'\n",
        "subdirectories = [name.replace('.zip', '') for name in os.listdir(input_directory) if name.endswith('.zip')]\n",
        "print(\"Please choose one corpus from the list:\", subdirectories)\n",
        "corpus = input(\"Enter your choice: \")\n",
        "input_directory = f'{input_directory}{corpus}.zip'\n",
        "emb = 'mbert'\n",
        "seed = '1'\n",
        "device = '0'\n",
        "\n",
        "lg = explanation_dict[language]\n",
        "lg_dict = {'ar': 'ar_padt', 'en': 'en_ewt', 'es': 'es_ancora', 'hr': 'hr_set', 'cs': 'cs_pdt', 'de': 'de_gsd', 'it':'it_partut', 'no': 'no_bokmaal', 'pt': 'pt_gsd', 'lv': 'lv_lvtb', 'is': 'is_modern', 'fi': 'fi_tdt', 'zh': 'zh_gsdsimp', 'ko': 'ko_kaist'}\n",
        "lg_code = lg_dict[lg]\n",
        "\n",
        "archived_output_directory = f'/content/drive/MyDrive/cvae_project/3_pred_format/{language}/{nativeness}/'\n",
        "output_directory = f'/content/{corpus}/'\n",
        "logs_dir = f\"/content/drive/MyDrive/crosslinguistic_nli/machamp/logs/{lg_code}/{emb}_{seed}/2022.03.09_21.47.34/model.tar.gz\"\n",
        "predict_dir = '/content/drive/MyDrive/crosslinguistic_nli/machamp/predict.py'\n",
        "os.makedirs(archived_output_directory, exist_ok=True)\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "archived_output_directory = archived_output_directory + f'{corpus}_machamp_{emb}_{seed}'\n",
        "print()\n",
        "print(f'input_directory: {input_directory}')\n",
        "print(f'output_directory: {output_directory}')\n",
        "print(f'archived_output_directory: {archived_output_directory}')\n",
        "print(f'logs_dir: {logs_dir}')\n",
        "print(f'predict_dir: {predict_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gvwvOoR6UdM",
        "outputId": "2c1901b0-e38e-4c81-d27b-b6edbd84ce29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: overrides\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10172 sha256=7c0bd95bae7294644af3629404c0669c464d788abacdb3ea4d132547819401fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/23/63/4d5849844f8f9d32be09e1b9b278e80de2d8314fbf1e28068b\n",
            "Successfully built overrides\n",
            "Installing collected packages: overrides\n",
            "Successfully installed overrides-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoN8F9zx44fw",
        "outputId": "40298e26-08df-414e-db54-a612491305f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<1.13.0,>=1.10.0 (from allennlp)\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.14.0,>=0.8.1 (from allennlp)\n",
            "  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting fairscale==0.4.6 (from allennlp)\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.8.1)\n",
            "Collecting spacy<3.4,>=2.1.0 (from allennlp)\n",
            "  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.23.5)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp) (4.66.1)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.10.1)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (7.4.0)\n",
            "Collecting transformers<4.21,>=4.1 (from allennlp)\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece>=0.1.96 (from allennlp)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock<3.8,>=3.3 (from allennlp)\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting lmdb>=1.2.1 (from allennlp)\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (10.1.0)\n",
            "Collecting termcolor==1.1.0 (from allennlp)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb<0.13.0,>=0.10.0 (from allennlp)\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.16 (from allennlp)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.4 (from allennlp)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting base58>=2.1.1 (from allennlp)\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting sacremoses (from allennlp)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.9.0)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.20.3)\n",
            "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (5.7.1)\n",
            "Collecting jsonnet>=0.10.0 (from allennlp)\n",
            "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading boto3-1.28.30-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m873.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.8.0)\n",
            "Collecting huggingface-hub>=0.0.16 (from allennlp)\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (23.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (2023.6.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
            "Collecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.10)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.9)\n",
            "Collecting typer>=0.4.1 (from allennlp)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (67.7.2)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.0.16->allennlp)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.4.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting botocore<1.32.0,>=1.31.30 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading botocore-1.31.30-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.5.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.30->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.28->allennlp)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.60.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.5.0)\n",
            "Building wheels for collected packages: fairscale, termcolor, jsonnet, sacremoses, pathtools\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307221 sha256=5d207a6b270003a3c582ef663b833ddb97e73e2563d15cfa4c128b2490b476df\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=7a3687dd0276eea3dc08d90219cf618f4823e697e1240c869cff5cca23350de5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406863 sha256=ee85eb7508e933b2f2e1788e3512e0d9200f1e50998a39308f6b3e49694722d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895240 sha256=2e5c3d07fcb03a48b0509372120df890ef3c5d432c488cf52cd74157be66c91f\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=dba726472ba016f77a7086f5d3e86f964356f495db1614d0fe612e1dd967eeac\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built fairscale termcolor jsonnet sacremoses pathtools\n",
            "Installing collected packages: wasabi, tokenizers, termcolor, sentencepiece, pathtools, lmdb, jsonnet, commonmark, urllib3, typing-extensions, typer, tensorboardX, smmap, shortuuid, setproctitle, sacremoses, rich, jmespath, filelock, docker-pycreds, dill, base58, torch, sentry-sdk, pydantic, gitdb, botocore, torchvision, thinc, s3transfer, huggingface-hub, GitPython, fairscale, wandb, transformers, spacy, boto3, cached-path, allennlp\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.3.0\n",
            "    Uninstalling termcolor-2.3.0:\n",
            "      Successfully uninstalled termcolor-2.3.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.5.2\n",
            "    Uninstalling rich-13.5.2:\n",
            "      Successfully uninstalled rich-13.5.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.12.2\n",
            "    Uninstalling filelock-3.12.2:\n",
            "      Successfully uninstalled filelock-3.12.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.1.1\n",
            "    Uninstalling pydantic-2.1.1:\n",
            "      Successfully uninstalled pydantic-2.1.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.3.3 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "pydantic-core 2.4.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.32 allennlp-2.10.1 base58-2.1.1 boto3-1.28.30 botocore-1.31.30 cached-path-1.1.6 commonmark-0.9.1 dill-0.3.7 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.10 huggingface-hub-0.10.1 jmespath-1.0.1 jsonnet-0.20.0 lmdb-1.4.1 pathtools-0.1.2 pydantic-1.8.2 rich-12.6.0 s3transfer-0.6.2 sacremoses-0.0.53 sentencepiece-0.1.99 sentry-sdk-1.29.2 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 typing-extensions-4.5.0 urllib3-1.26.16 wandb-0.12.21 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install allennlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# delete unnecessary temporary directory\n",
        "def delete_directory(directory):\n",
        "    for root, dirs, files in os.walk(directory, topdown=False):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            os.remove(file_path)\n",
        "        for dir in dirs:\n",
        "            dir_path = os.path.join(root, dir)\n",
        "            os.rmdir(dir_path)\n",
        "    os.rmdir(directory)\n"
      ],
      "metadata": {
        "id": "kPiIomatBcoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip input files\n",
        "input_extended_essays_path = f'/content/concatenated_essays/{corpus}'\n",
        "input_extraction_path = f'/content/original_essays/{corpus}'\n",
        "\n",
        "shutil.unpack_archive(input_directory, input_extraction_path, 'zip')\n",
        "os.makedirs(input_extended_essays_path, exist_ok=True)\n",
        "\n",
        "# unzip parsed files so that you can continue parsing\n",
        "splitted_output = output_directory + 'splitted/'\n",
        "os.makedirs(splitted_output, exist_ok=True)\n",
        "\n",
        "if os.path.exists(archived_output_directory+'.zip'):\n",
        "    shutil.unpack_archive(archived_output_directory+'.zip', splitted_output, 'zip')\n",
        "\n",
        "\n",
        "# concatenate short essays into long essays before parsing\n",
        "count = 0\n",
        "total_count = 0\n",
        "all_content = ''\n",
        "essay_breakers = []\n",
        "essay_breaker = {}\n",
        "start_index = 0\n",
        "out_index = 0\n",
        "parsed_list = os.listdir(splitted_output)\n",
        "\n",
        "for file_name in tqdm(os.listdir(input_extraction_path)):\n",
        "    if file_name.endswith('.conllu') and file_name.replace('.conllu', '.pred') not in parsed_list:\n",
        "        count += 1\n",
        "        with open(f'{input_extraction_path}/{file_name}', 'r') as f:\n",
        "            file_content = f.read()\n",
        "        line_count = file_content.count(\"\\n\")\n",
        "        essay_breaker[file_name] = (start_index, start_index+line_count+2)\n",
        "        start_index = start_index + line_count + 2\n",
        "        all_content = all_content + file_content + '\\n' + '\\n'\n",
        "\n",
        "        if count == concatenation_batch:\n",
        "            total_count += count\n",
        "            count = 0\n",
        "            out_index += 1\n",
        "            essay_breakers.append(essay_breaker)\n",
        "            essay_breaker = {}\n",
        "            with open(f'{input_extended_essays_path}/{out_index}.conllu', 'w') as f:\n",
        "                f.write(all_content)\n",
        "            all_content = ''\n",
        "            start_index = 0\n",
        "\n",
        "if count != 0:\n",
        "    total_count += count\n",
        "    out_index += 1\n",
        "    essay_breakers.append(essay_breaker)\n",
        "    with open(f'{input_extended_essays_path}/{out_index}.conllu', 'w') as f:\n",
        "        f.write(all_content)\n",
        "\n",
        "print(f'\\n{total_count} number of essays will be parsed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KohAARF3AIX",
        "outputId": "5e1f37f4-e829-4d2a-c1d8-5607b9b53654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2481/2481 [00:01<00:00, 1756.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2481 number of essays will be parsed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delete_directory('/content/original_essays/')"
      ],
      "metadata": {
        "id": "R5aM5EiKDUSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_substring_between_newlines(text, n, m):\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    if n < 0 or m < n or m > len(lines):\n",
        "        raise ValueError(f\"Invalid input(likely to be caused by GPU out of memory): n: {n}, m: {m}, length of text: {len(lines)}\")\n",
        "\n",
        "    substring = '\\n'.join(lines[n:m])\n",
        "    return substring\n",
        "\n",
        "# Example usage\n",
        "# input_text = \"Line 1\\nLine 2\\nLine 3\\nLine 4\\nLine 5\\nLine 6\\nLine 7\"\n",
        "# n_value = 2\n",
        "# m_value = 5\n",
        "# result = extract_substring_between_newlines(input_text, n_value, m_value)\n",
        "# print(result)\n"
      ],
      "metadata": {
        "id": "WTo-adfX8ocG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = output_directory + 'concatenated/'\n",
        "os.makedirs(output_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "BKknmii2vWm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhjK6Q0Ym8ro",
        "outputId": "b7559213-a2f7-4b69-a77f-48087cc20801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-20 21:18:52.044706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 21:18:55.484649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-20 21:19:20,817 - INFO - allennlp.models.archival - loading archive file /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34 from cache at /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34\n",
            "2023-08-20 21:19:20,864 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:19:20,864 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:19:20,864 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:19:20,865 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:19:20,865 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:19:20,865 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:19:20,865 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 203kB/s]\n",
            "Downloading: 100% 625/625 [00:00<00:00, 4.33MB/s]\n",
            "Downloading: 100% 972k/972k [00:00<00:00, 5.99MB/s]\n",
            "Downloading: 100% 1.87M/1.87M [00:00<00:00, 4.70MB/s]\n",
            "2023-08-20 21:19:26,436 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:19:26,436 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:19:26,436 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:19:26,436 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:19:26,436 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:19:26,438 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:19:26,439 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:19:26,440 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:19:26,440 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:19:26,440 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:19:26,440 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:19:26,440 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:19:26,440 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:19:26,440 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:19:26,441 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:19:26,442 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:19:26,442 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:19:26,442 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:19:26,442 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:19:26,443 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:19:26,444 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:19:26,445 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:19:26,445 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2023-08-20 21:19:26,445 - INFO - allennlp.data.vocabulary - Loading token dictionary from /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34/vocabulary.\n",
            "2023-08-20 21:19:27,926 - INFO - allennlp.common.params - model.type = machamp_model\n",
            "2023-08-20 21:19:27,927 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2023-08-20 21:19:27,927 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2023-08-20 21:19:27,927 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2023-08-20 21:19:27,928 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = machamp_pretrained_transformer_mismatched\n",
            "2023-08-20 21:19:27,928 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:19:27,928 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 128\n",
            "2023-08-20 21:19:27,928 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2023-08-20 21:19:27,928 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.layers_to_use = [-1]\n",
            "2023-08-20 21:19:27,928 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "Downloading: 100% 681M/681M [00:09<00:00, 76.3MB/s]\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2023-08-20 21:19:40,819 - INFO - allennlp.common.params - model.encoder.type = cls_pooler\n",
            "2023-08-20 21:19:40,819 - INFO - allennlp.common.params - model.encoder.embedding_dim = 768\n",
            "2023-08-20 21:19:40,819 - INFO - allennlp.common.params - model.encoder.cls_is_last_token = False\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.type = machamp_dependency_decoder\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.regularizer = None\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.ddp_accelerator = None\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.task = dependency\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.input_dim = 768\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.tag_representation_dim = 256\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.arc_representation_dim = 768\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.loss_weight = 1\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.metric = las\n",
            "2023-08-20 21:19:40,820 - INFO - allennlp.common.params - model.decoders.dependency.use_mst_decoding_for_validation = True\n",
            "2023-08-20 21:19:40,821 - INFO - allennlp.common.params - model.decoders.dependency.dataset_embeds_dim = 0\n",
            "2023-08-20 21:19:40,854 - INFO - allennlp.common.params - model.tasks = ['dependency']\n",
            "2023-08-20 21:19:40,854 - INFO - allennlp.common.params - model.task_types = ['dependency']\n",
            "2023-08-20 21:19:40,854 - INFO - allennlp.common.params - model.dataset_embeds_dim = 0\n",
            "2023-08-20 21:19:40,854 - INFO - allennlp.common.params - model.dropout = 0.3\n",
            "2023-08-20 21:19:40,854 - INFO - allennlp.common.params - model.dataset_embedder = None\n",
            "2023-08-20 21:19:56,239 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_rels'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:19:56,239 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_head_indices'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:19:56,239 - WARNING - allennlp.data.fields.label_field - Your label namespace was 'dataset'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:20:00,676 - WARNING - allennlp.models.model - Encountered the logits key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:20:00,676 - WARNING - allennlp.models.model - Encountered the class_probabilities key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:20:00,676 - WARNING - allennlp.models.model - Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "/content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/machamp/modules/pretrained_transformer_embedder.py:339: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
            "{'.run/.sum': 0.842112596252984, '.run/dependency/las': 0.842112596252984}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  20%|██        | 1/5 [02:14<08:59, 134.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-20 21:21:00.627611: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 21:21:01.916894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-20 21:21:06,018 - INFO - allennlp.models.archival - loading archive file /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34 from cache at /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34\n",
            "2023-08-20 21:21:06,100 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:21:06,101 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:21:06,101 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:21:06,101 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:21:06,101 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:21:06,101 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:21:06,101 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:21:11,151 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:21:11,152 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:21:11,152 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:21:11,152 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:21:11,152 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:21:11,153 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:21:11,154 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:21:11,154 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:21:11,154 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:21:11,154 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:21:11,154 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:21:11,154 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:21:11,154 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:21:11,155 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:21:11,156 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:21:11,156 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:21:11,156 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:21:11,156 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:21:11,157 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:21:11,157 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:21:11,157 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:21:11,157 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:21:11,157 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:21:11,158 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:21:11,158 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:21:11,158 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:21:11,158 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:21:11,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:21:11,160 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:21:11,160 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:21:11,160 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:21:11,160 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:21:11,160 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:21:11,160 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:21:11,160 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:21:11,161 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:21:11,161 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:21:11,161 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:21:11,161 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:21:11,161 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:21:11,162 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:21:11,162 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:21:11,162 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:21:11,162 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:21:11,162 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:21:11,162 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:21:11,162 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2023-08-20 21:21:11,163 - INFO - allennlp.data.vocabulary - Loading token dictionary from /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34/vocabulary.\n",
            "2023-08-20 21:21:11,173 - INFO - allennlp.common.params - model.type = machamp_model\n",
            "2023-08-20 21:21:11,174 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2023-08-20 21:21:11,174 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2023-08-20 21:21:11,175 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2023-08-20 21:21:11,175 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = machamp_pretrained_transformer_mismatched\n",
            "2023-08-20 21:21:11,175 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:21:11,176 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 128\n",
            "2023-08-20 21:21:11,176 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2023-08-20 21:21:11,176 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.layers_to_use = [-1]\n",
            "2023-08-20 21:21:11,176 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2023-08-20 21:21:15,605 - INFO - allennlp.common.params - model.encoder.type = cls_pooler\n",
            "2023-08-20 21:21:15,605 - INFO - allennlp.common.params - model.encoder.embedding_dim = 768\n",
            "2023-08-20 21:21:15,605 - INFO - allennlp.common.params - model.encoder.cls_is_last_token = False\n",
            "2023-08-20 21:21:15,606 - INFO - allennlp.common.params - model.decoders.dependency.type = machamp_dependency_decoder\n",
            "2023-08-20 21:21:15,606 - INFO - allennlp.common.params - model.decoders.dependency.regularizer = None\n",
            "2023-08-20 21:21:15,606 - INFO - allennlp.common.params - model.decoders.dependency.ddp_accelerator = None\n",
            "2023-08-20 21:21:15,606 - INFO - allennlp.common.params - model.decoders.dependency.task = dependency\n",
            "2023-08-20 21:21:15,606 - INFO - allennlp.common.params - model.decoders.dependency.input_dim = 768\n",
            "2023-08-20 21:21:15,606 - INFO - allennlp.common.params - model.decoders.dependency.tag_representation_dim = 256\n",
            "2023-08-20 21:21:15,606 - INFO - allennlp.common.params - model.decoders.dependency.arc_representation_dim = 768\n",
            "2023-08-20 21:21:15,607 - INFO - allennlp.common.params - model.decoders.dependency.loss_weight = 1\n",
            "2023-08-20 21:21:15,607 - INFO - allennlp.common.params - model.decoders.dependency.metric = las\n",
            "2023-08-20 21:21:15,607 - INFO - allennlp.common.params - model.decoders.dependency.use_mst_decoding_for_validation = True\n",
            "2023-08-20 21:21:15,607 - INFO - allennlp.common.params - model.decoders.dependency.dataset_embeds_dim = 0\n",
            "2023-08-20 21:21:15,642 - INFO - allennlp.common.params - model.tasks = ['dependency']\n",
            "2023-08-20 21:21:15,642 - INFO - allennlp.common.params - model.task_types = ['dependency']\n",
            "2023-08-20 21:21:15,642 - INFO - allennlp.common.params - model.dataset_embeds_dim = 0\n",
            "2023-08-20 21:21:15,642 - INFO - allennlp.common.params - model.dropout = 0.3\n",
            "2023-08-20 21:21:15,642 - INFO - allennlp.common.params - model.dataset_embedder = None\n",
            "2023-08-20 21:21:20,041 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_rels'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:21:20,041 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_head_indices'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:21:20,041 - WARNING - allennlp.data.fields.label_field - Your label namespace was 'dataset'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:21:22,281 - WARNING - allennlp.models.model - Encountered the logits key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:21:22,281 - WARNING - allennlp.models.model - Encountered the class_probabilities key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:21:22,281 - WARNING - allennlp.models.model - Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "/content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/machamp/modules/pretrained_transformer_embedder.py:339: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
            "{'.run/.sum': 0.8427923620906741, '.run/dependency/las': 0.8427923620906741}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  40%|████      | 2/5 [03:33<05:05, 101.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-20 21:22:19.159233: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 21:22:20.216069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-20 21:22:23,804 - INFO - allennlp.models.archival - loading archive file /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34 from cache at /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34\n",
            "2023-08-20 21:22:23,879 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:22:23,879 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:22:23,880 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:22:23,880 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:22:23,880 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:22:23,880 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:22:23,880 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:22:29,744 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:22:29,744 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:22:29,744 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:22:29,744 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:22:29,744 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:22:29,746 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:22:29,747 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:22:29,748 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:22:29,748 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:22:29,748 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:22:29,748 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:22:29,749 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:22:29,749 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:22:29,749 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:22:29,749 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:22:29,749 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:22:29,750 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:22:29,750 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:22:29,750 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:22:29,750 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:22:29,751 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:22:29,752 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:22:29,753 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:22:29,754 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2023-08-20 21:22:29,754 - INFO - allennlp.data.vocabulary - Loading token dictionary from /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34/vocabulary.\n",
            "2023-08-20 21:22:29,769 - INFO - allennlp.common.params - model.type = machamp_model\n",
            "2023-08-20 21:22:29,770 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2023-08-20 21:22:29,770 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2023-08-20 21:22:29,770 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2023-08-20 21:22:29,770 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = machamp_pretrained_transformer_mismatched\n",
            "2023-08-20 21:22:29,771 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:22:29,771 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 128\n",
            "2023-08-20 21:22:29,771 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2023-08-20 21:22:29,771 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.layers_to_use = [-1]\n",
            "2023-08-20 21:22:29,771 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2023-08-20 21:22:33,202 - INFO - allennlp.common.params - model.encoder.type = cls_pooler\n",
            "2023-08-20 21:22:33,203 - INFO - allennlp.common.params - model.encoder.embedding_dim = 768\n",
            "2023-08-20 21:22:33,203 - INFO - allennlp.common.params - model.encoder.cls_is_last_token = False\n",
            "2023-08-20 21:22:33,203 - INFO - allennlp.common.params - model.decoders.dependency.type = machamp_dependency_decoder\n",
            "2023-08-20 21:22:33,203 - INFO - allennlp.common.params - model.decoders.dependency.regularizer = None\n",
            "2023-08-20 21:22:33,203 - INFO - allennlp.common.params - model.decoders.dependency.ddp_accelerator = None\n",
            "2023-08-20 21:22:33,203 - INFO - allennlp.common.params - model.decoders.dependency.task = dependency\n",
            "2023-08-20 21:22:33,203 - INFO - allennlp.common.params - model.decoders.dependency.input_dim = 768\n",
            "2023-08-20 21:22:33,204 - INFO - allennlp.common.params - model.decoders.dependency.tag_representation_dim = 256\n",
            "2023-08-20 21:22:33,204 - INFO - allennlp.common.params - model.decoders.dependency.arc_representation_dim = 768\n",
            "2023-08-20 21:22:33,204 - INFO - allennlp.common.params - model.decoders.dependency.loss_weight = 1\n",
            "2023-08-20 21:22:33,204 - INFO - allennlp.common.params - model.decoders.dependency.metric = las\n",
            "2023-08-20 21:22:33,204 - INFO - allennlp.common.params - model.decoders.dependency.use_mst_decoding_for_validation = True\n",
            "2023-08-20 21:22:33,204 - INFO - allennlp.common.params - model.decoders.dependency.dataset_embeds_dim = 0\n",
            "2023-08-20 21:22:33,237 - INFO - allennlp.common.params - model.tasks = ['dependency']\n",
            "2023-08-20 21:22:33,237 - INFO - allennlp.common.params - model.task_types = ['dependency']\n",
            "2023-08-20 21:22:33,237 - INFO - allennlp.common.params - model.dataset_embeds_dim = 0\n",
            "2023-08-20 21:22:33,237 - INFO - allennlp.common.params - model.dropout = 0.3\n",
            "2023-08-20 21:22:33,237 - INFO - allennlp.common.params - model.dataset_embedder = None\n",
            "2023-08-20 21:22:37,266 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_rels'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:22:37,267 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_head_indices'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:22:37,267 - WARNING - allennlp.data.fields.label_field - Your label namespace was 'dataset'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:22:39,402 - WARNING - allennlp.models.model - Encountered the logits key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:22:39,402 - WARNING - allennlp.models.model - Encountered the class_probabilities key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:22:39,403 - WARNING - allennlp.models.model - Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "/content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/machamp/modules/pretrained_transformer_embedder.py:339: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
            "{'.run/.sum': 0.845027969118395, '.run/dependency/las': 0.845027969118395}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  60%|██████    | 3/5 [04:51<03:01, 90.84s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-20 21:23:36.936891: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 21:23:38.087074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-20 21:23:42,694 - INFO - allennlp.models.archival - loading archive file /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34 from cache at /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34\n",
            "2023-08-20 21:23:42,776 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:23:42,777 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:23:42,777 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:23:42,777 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:23:42,777 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:23:42,777 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:23:42,778 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:23:47,804 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:23:47,805 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:23:47,805 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:23:47,805 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:23:47,805 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:23:47,806 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:23:47,807 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:23:47,807 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:23:47,807 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:23:47,807 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:23:47,807 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:23:47,807 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:23:47,807 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:23:47,808 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:23:47,809 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:23:47,809 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:23:47,809 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:23:47,809 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:23:47,810 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:23:47,810 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:23:47,810 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:23:47,810 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:23:47,810 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:23:47,811 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:23:47,811 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:23:47,812 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:23:47,812 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:23:47,812 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:23:47,813 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:23:47,814 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:23:47,814 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:23:47,814 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:23:47,814 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:23:47,814 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:23:47,815 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:23:47,815 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:23:47,815 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:23:47,815 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:23:47,815 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:23:47,815 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2023-08-20 21:23:47,816 - INFO - allennlp.data.vocabulary - Loading token dictionary from /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34/vocabulary.\n",
            "2023-08-20 21:23:47,827 - INFO - allennlp.common.params - model.type = machamp_model\n",
            "2023-08-20 21:23:47,828 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2023-08-20 21:23:47,828 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2023-08-20 21:23:47,829 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2023-08-20 21:23:47,829 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = machamp_pretrained_transformer_mismatched\n",
            "2023-08-20 21:23:47,830 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:23:47,830 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 128\n",
            "2023-08-20 21:23:47,830 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2023-08-20 21:23:47,830 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.layers_to_use = [-1]\n",
            "2023-08-20 21:23:47,830 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2023-08-20 21:23:51,202 - INFO - allennlp.common.params - model.encoder.type = cls_pooler\n",
            "2023-08-20 21:23:51,202 - INFO - allennlp.common.params - model.encoder.embedding_dim = 768\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.encoder.cls_is_last_token = False\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.type = machamp_dependency_decoder\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.regularizer = None\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.ddp_accelerator = None\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.task = dependency\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.input_dim = 768\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.tag_representation_dim = 256\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.arc_representation_dim = 768\n",
            "2023-08-20 21:23:51,203 - INFO - allennlp.common.params - model.decoders.dependency.loss_weight = 1\n",
            "2023-08-20 21:23:51,204 - INFO - allennlp.common.params - model.decoders.dependency.metric = las\n",
            "2023-08-20 21:23:51,204 - INFO - allennlp.common.params - model.decoders.dependency.use_mst_decoding_for_validation = True\n",
            "2023-08-20 21:23:51,204 - INFO - allennlp.common.params - model.decoders.dependency.dataset_embeds_dim = 0\n",
            "2023-08-20 21:23:51,237 - INFO - allennlp.common.params - model.tasks = ['dependency']\n",
            "2023-08-20 21:23:51,237 - INFO - allennlp.common.params - model.task_types = ['dependency']\n",
            "2023-08-20 21:23:51,237 - INFO - allennlp.common.params - model.dataset_embeds_dim = 0\n",
            "2023-08-20 21:23:51,237 - INFO - allennlp.common.params - model.dropout = 0.3\n",
            "2023-08-20 21:23:51,237 - INFO - allennlp.common.params - model.dataset_embedder = None\n",
            "2023-08-20 21:23:55,373 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_rels'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:23:55,373 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_head_indices'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:23:55,374 - WARNING - allennlp.data.fields.label_field - Your label namespace was 'dataset'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:23:57,514 - WARNING - allennlp.models.model - Encountered the logits key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:23:57,514 - WARNING - allennlp.models.model - Encountered the class_probabilities key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:23:57,514 - WARNING - allennlp.models.model - Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "/content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/machamp/modules/pretrained_transformer_embedder.py:339: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
            "{'.run/.sum': 0.8410241712047387, '.run/dependency/las': 0.8410241712047387}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing:  80%|████████  | 4/5 [06:06<01:24, 84.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-20 21:24:52.451800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 21:24:53.542903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-20 21:24:57,022 - INFO - allennlp.models.archival - loading archive file /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34 from cache at /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34\n",
            "2023-08-20 21:24:57,096 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:24:57,096 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:24:57,097 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:24:57,097 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:24:57,097 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:24:57,097 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:24:57,097 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:25:02,118 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:25:02,118 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:25:02,118 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:25:02,118 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:25:02,119 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:25:02,120 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:25:02,120 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:25:02,120 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:25:02,120 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:25:02,120 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:25:02,120 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:25:02,120 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:25:02,121 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:25:02,121 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:25:02,121 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:25:02,121 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:25:02,122 - INFO - allennlp.common.params - dataset_reader.type = machamp_universal_reader\n",
            "2023-08-20 21:25:02,123 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2023-08-20 21:25:02,123 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:25:02,123 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = False\n",
            "2023-08-20 21:25:02,123 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n",
            "2023-08-20 21:25:02,123 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2023-08-20 21:25:02,123 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2023-08-20 21:25:02,124 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mixmatched\n",
            "2023-08-20 21:25:02,125 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:25:02,125 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:25:02,125 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2023-08-20 21:25:02,125 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 128\n",
            "2023-08-20 21:25:02,126 - INFO - allennlp.common.params - dataset_reader.is_raw = False\n",
            "2023-08-20 21:25:02,126 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.column_idx = 6\n",
            "2023-08-20 21:25:02,126 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.tasks.dependency.task_type = dependency\n",
            "2023-08-20 21:25:02,126 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.train_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "2023-08-20 21:25:02,126 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.validation_data_path = ../UD_data/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "2023-08-20 21:25:02,126 - INFO - allennlp.common.params - dataset_reader.datasets.English-EWT.word_idx = 1\n",
            "2023-08-20 21:25:02,127 - INFO - allennlp.common.params - dataset_reader.do_lowercase = False\n",
            "2023-08-20 21:25:02,127 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = bert_basic_tokenizer\n",
            "2023-08-20 21:25:02,127 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2023-08-20 21:25:02,127 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_words\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.feature_name = text\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.default_value = THIS IS A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.token_min_padding_length = 0\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.source_max_tokens = 128\n",
            "2023-08-20 21:25:02,128 - INFO - allennlp.common.params - dataset_reader.target_max_tokens = 128\n",
            "2023-08-20 21:25:02,129 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2023-08-20 21:25:02,129 - INFO - allennlp.data.vocabulary - Loading token dictionary from /content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/logs/en_ewt/mbert_1/2022.03.09_21.47.34/vocabulary.\n",
            "2023-08-20 21:25:02,140 - INFO - allennlp.common.params - model.type = machamp_model\n",
            "2023-08-20 21:25:02,141 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2023-08-20 21:25:02,142 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2023-08-20 21:25:02,142 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2023-08-20 21:25:02,142 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = machamp_pretrained_transformer_mismatched\n",
            "2023-08-20 21:25:02,143 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = bert-base-multilingual-cased\n",
            "2023-08-20 21:25:02,143 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 128\n",
            "2023-08-20 21:25:02,143 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2023-08-20 21:25:02,143 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.layers_to_use = [-1]\n",
            "2023-08-20 21:25:02,143 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2023-08-20 21:25:06,592 - INFO - allennlp.common.params - model.encoder.type = cls_pooler\n",
            "2023-08-20 21:25:06,592 - INFO - allennlp.common.params - model.encoder.embedding_dim = 768\n",
            "2023-08-20 21:25:06,592 - INFO - allennlp.common.params - model.encoder.cls_is_last_token = False\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.type = machamp_dependency_decoder\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.regularizer = None\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.ddp_accelerator = None\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.task = dependency\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.input_dim = 768\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.tag_representation_dim = 256\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.arc_representation_dim = 768\n",
            "2023-08-20 21:25:06,593 - INFO - allennlp.common.params - model.decoders.dependency.loss_weight = 1\n",
            "2023-08-20 21:25:06,594 - INFO - allennlp.common.params - model.decoders.dependency.metric = las\n",
            "2023-08-20 21:25:06,594 - INFO - allennlp.common.params - model.decoders.dependency.use_mst_decoding_for_validation = True\n",
            "2023-08-20 21:25:06,594 - INFO - allennlp.common.params - model.decoders.dependency.dataset_embeds_dim = 0\n",
            "2023-08-20 21:25:06,629 - INFO - allennlp.common.params - model.tasks = ['dependency']\n",
            "2023-08-20 21:25:06,629 - INFO - allennlp.common.params - model.task_types = ['dependency']\n",
            "2023-08-20 21:25:06,629 - INFO - allennlp.common.params - model.dataset_embeds_dim = 0\n",
            "2023-08-20 21:25:06,629 - INFO - allennlp.common.params - model.dropout = 0.3\n",
            "2023-08-20 21:25:06,630 - INFO - allennlp.common.params - model.dataset_embedder = None\n",
            "2023-08-20 21:25:10,811 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_rels'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:25:10,811 - WARNING - allennlp.data.fields.sequence_label_field - Your label namespace was 'dependency_head_indices'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:25:10,811 - WARNING - allennlp.data.fields.label_field - Your label namespace was 'dataset'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
            "2023-08-20 21:25:12,970 - WARNING - allennlp.models.model - Encountered the logits key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:25:12,970 - WARNING - allennlp.models.model - Encountered the class_probabilities key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "2023-08-20 21:25:12,970 - WARNING - allennlp.models.model - Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
            "/content/drive/.shortcut-targets-by-id/1juATzWwdx5yPo8nTk3MDAmvLzh78gduZ/crosslinguistic_nli/machamp/machamp/modules/pretrained_transformer_embedder.py:339: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
            "{'.run/.sum': 0.8427720788736575, '.run/dependency/las': 0.8427720788736575}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 5/5 [07:24<00:00, 88.95s/it]\n"
          ]
        }
      ],
      "source": [
        "# parsing.... takes about 2 to 40 mins each corpora dependending on the size\n",
        "# remember to check the error message that may say GPU out of memory\n",
        "\n",
        "for filename in tqdm(os.listdir(input_extended_essays_path), desc='Processing'):\n",
        "    out_name = filename.replace('.conllu', '.pred')\n",
        "    in_dir = os.path.join(input_extended_essays_path, filename)\n",
        "    out_dir = os.path.join(output_path, out_name)\n",
        "    !python3 {predict_dir} {logs_dir} {in_dir} {out_dir} --device 0 --batch_size 16 --device {device}\n",
        "    index = int(filename.replace('.conllu', ''))\n",
        "    essay_breaker = essay_breakers[index-1]\n",
        "    with open(output_path+f'{index}.pred', 'r') as f:\n",
        "        file_content = f.read()\n",
        "    for key, value in essay_breaker.items():\n",
        "        essay = extract_substring_between_newlines(file_content, value[0], value[1])\n",
        "        if essay.startswith('1\\t') == False:\n",
        "            print(essay)\n",
        "            raise ValueError(f'Incorrect segmentation of essay {key}, {value} in {index}.pred')\n",
        "        with open(splitted_output+key.replace('.conllu', '.pred'), 'w') as f:\n",
        "            f.write(essay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYfqbzTCzvnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cd3bfe1a-cc9c-42f6-b428-beafd6edc622"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/cvae_project/3_pred_format/English/Learner/CLC_machamp_mbert_1.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# archive output files to speed up transmission through API\n",
        "if os.path.exists(archived_output_directory+'.zip'):\n",
        "    os.remove(archived_output_directory+'.zip')\n",
        "shutil.make_archive(archived_output_directory, 'zip', splitted_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfezNRqeGX2W"
      },
      "outputs": [],
      "source": [
        "# Call the function to clear the /content/ directory after using the unzipped files\n",
        "def clear_content_directory():\n",
        "    content_dir = '/content/'\n",
        "    exempt_folders = ['drive', 'sample_data']\n",
        "\n",
        "    for item in os.listdir(content_dir):\n",
        "        item_path = os.path.join(content_dir, item)\n",
        "        if os.path.isdir(item_path) and item not in exempt_folders:\n",
        "            for root, dirs, files in os.walk(item_path, topdown=False):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    os.remove(file_path)\n",
        "                for dir in dirs:\n",
        "                    dir_path = os.path.join(root, dir)\n",
        "                    os.rmdir(dir_path)\n",
        "            os.rmdir(item_path)\n",
        "        elif os.path.isfile(item_path):\n",
        "            os.remove(item_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNKGlIZ6GGbp"
      },
      "outputs": [],
      "source": [
        "# clear temporary /content/ dir\n",
        "clear_content_directory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u487msu3WVf9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}