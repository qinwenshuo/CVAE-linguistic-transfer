{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qinwenshuo/CVAE-linguistic-transfer/blob/main/3_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekaGZ9-vJksu"
      },
      "source": [
        "# Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lng9FwwMWoUP"
      },
      "outputs": [],
      "source": [
        "# features removed:\n",
        "\n",
        "# All evident related features(10)\n",
        "# All polarity related features(10)\n",
        "# All polite related features(14)\n",
        "# All clusivity related features(10)\n",
        "# All auxilary verb aspect related features(9)\n",
        "# All voice distribution features(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6s_bwQRTyht",
        "outputId": "db154458-404b-432c-f755-c64852b01b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSUg8wQPV9fh"
      },
      "outputs": [],
      "source": [
        "import os, random, statistics, math, re, shutil, csv\n",
        "import networkx as nx\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crszA3WvT175",
        "outputId": "b51baeb5-d512-4198-8820-5208199ad625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please choose one language from the list: ['Arabic', 'English', 'Spanish', 'Croatian', 'Czech', 'German', 'Italian', 'Norwegian', 'Portuguese', 'Latvian', 'Icelandic', 'Finnish', 'Chinese', 'Korean']\n",
            "Enter your choice: English\n",
            "Native or Learner?\n",
            "Enter your choice: Learner\n",
            "Please choose one input corpus from the list: ['ArabCC_machamp_mbert_1', 'ICNALE_machamp_mbert_1', 'WriCLE_machamp_mbert_1', 'ICLE_machamp_mbert_1', 'PELIC_machamp_mbert_1', 'BAWE_machamp_mbert_1', 'TOEFL_machamp_mbert_1', 'Gachon_machamp_mbert_1', 'CLC_machamp_mbert_1', 'WriCLE-informal_machamp_mbert_1']\n",
            "Enter your choice: Gachon_machamp_mbert_1\n",
            "\n",
            "Input directory: /content/drive/MyDrive/cvae_project/3_pred_format/English/Learner/Gachon_machamp_mbert_1.zip\n",
            "Input raw text directory: /content/drive/MyDrive/cvae_project/1_extract_txt_format/English/Learner/Gachon.zip\n",
            "Output directory: /content/drive/MyDrive/cvae_project/4_dep_features/English/Learner/Gachon_machamp_mbert_1.txt\n"
          ]
        }
      ],
      "source": [
        "explanation_dict = {\n",
        "    'Arabic': 'ar',\n",
        "    'English': 'en',\n",
        "    'Spanish': 'es',\n",
        "    'Croatian': 'hr',\n",
        "    'Czech': 'cs',\n",
        "    'German': 'de',\n",
        "    'Italian': 'it',\n",
        "    'Norwegian': 'no',\n",
        "    'Portuguese': 'pt',\n",
        "    'Latvian': 'lv',\n",
        "    'Icelandic': 'is',\n",
        "    'Finnish': 'fi',\n",
        "    'Chinese': 'zh',\n",
        "    'Korean': 'ko'\n",
        "}\n",
        "\n",
        "\n",
        "language = input(f\"Please choose one language from the list: {list(explanation_dict.keys())}\\nEnter your choice: \")\n",
        "nativeness = input(\"Native or Learner?\\nEnter your choice: \")\n",
        "input_directory = f'/content/drive/MyDrive/cvae_project/3_pred_format/{language}/{nativeness}/'\n",
        "subdirectories = os.listdir(input_directory)\n",
        "subdirectories = [i.replace('.zip', '') for i in subdirectories if i.endswith('.zip')]\n",
        "corpus = input(f\"Please choose one input corpus from the list: {subdirectories}\\nEnter your choice: \")\n",
        "input_directory = os.path.join(input_directory, corpus+'.zip')\n",
        "corpus_name = corpus.rsplit('_', 3)[0]\n",
        "raw_text_input_directory = f'/content/drive/MyDrive/cvae_project/1_extract_txt_format/{language}/{nativeness}/{corpus_name}.zip'\n",
        "output_directory = f'/content/drive/MyDrive/cvae_project/4_dep_features/{language}/{nativeness}/'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "output_directory += f'{corpus}.txt'\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Input directory: {input_directory}\")\n",
        "print(f\"Input raw text directory: {raw_text_input_directory}\")\n",
        "print(f\"Output directory: {output_directory}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkJP8uPlOXqm"
      },
      "outputs": [],
      "source": [
        "first_language_filtered = ['Unknown', 'Filipino', 'Other', 'Bosnian-croatian-serbian', 'Pakistan', 'Malay', 'Philippines', 'Taiwanese']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nR0wq4DGnzk"
      },
      "source": [
        "# Morphological feature analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiZJvx0IGmYf"
      },
      "outputs": [],
      "source": [
        "function_POS = ['ADP', 'AUX', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ']\n",
        "clauses = ['csubj', 'ccomp', 'xcomp', 'advcl', 'acl']\n",
        "verb_features = ['Mood', 'Number', 'Person', 'Tense', 'VerbForm']\n",
        "\n",
        "verb_mood_features = ['Ind', 'Imp', 'Cnd', 'Pot', 'Sub', 'Jus', 'Prp', 'Qot', 'Opt', 'Des', 'Nec', 'Irr', 'Adm']\n",
        "verb_number_features = ['Sing', 'Plur', 'Dual', 'Tri', 'Pau', 'Grpa', 'Grpl', 'Inv', 'Count', 'Ptan', 'Coll']\n",
        "verb_tense_features = ['Past', 'Pres', 'Fut', 'Imp', 'Pqp']\n",
        "verb_form_features = ['Fin', 'Inf', 'Sup', 'Part', 'Conv', 'Gdv', 'Ger', 'Vnoun']\n",
        "verb_person_features = ['0', '1', '2', '3', '4']\n",
        "verb_voice_features = ['Act', 'Mid', 'Rcp', 'Pass', 'Antip', 'Lfoc', 'Bfoc', 'Dir', 'Inv', 'Cau']\n",
        "verb_aspect_features = ['Imp', 'Perf', 'Prosp', 'Prog', 'Hab', 'Iter']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC3qr_BxWEFe"
      },
      "outputs": [],
      "source": [
        "#  reads a sentence from a CoNLL file, validates the format and completeness\n",
        "#  of the annotations, and returns the sentence as a list of token-annotation pairs.\n",
        "\n",
        "# I wrote this new method, the previous version wrote by Zoey seems to have\n",
        "# errors(not able to read the last line in a file)\n",
        "\n",
        "def read_conllu_file(file_path):\n",
        "\t\tresult = []\n",
        "\t\tsentence = []\n",
        "\n",
        "\t\twith open(file_path, 'r', encoding='utf-8') as f:\n",
        "\t\t\t\tfor line in f:\n",
        "\t\t\t\t\t\tline = line.strip()\n",
        "\t\t\t\t\t\tif line == '':\n",
        "\t\t\t\t\t\t\t\tif sentence:\n",
        "\t\t\t\t\t\t\t\t\t\tresult.append(sentence)\n",
        "\t\t\t\t\t\t\t\t\t\tsentence = []\n",
        "\t\t\t\t\t\telif not line.startswith('#'):\n",
        "\t\t\t\t\t\t\t\telements = line.split('\\t')\n",
        "\t\t\t\t\t\t\t\tif len(elements) != 10 and len(elements) != 0:\n",
        "\t\t\t\t\t\t\t\t\t\traise ValueError (f\"Error: Line has {len(elements)} elements instead of 10 or 0: {line}\")\n",
        "\t\t\t\t\t\t\t\telif '-' in elements[0] or '.' in elements[0]:\n",
        "\t\t\t\t\t\t\t\t\t\tprint(f'Wired line: {elements}')\n",
        "\t\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\t\t\tsentence.append(elements)\n",
        "\n",
        "\t\tif sentence:\n",
        "\t\t\t\tresult.append(sentence)\n",
        "\n",
        "\t\treturn result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7p7N4DRaxJY"
      },
      "outputs": [],
      "source": [
        "def distribution(data_dict):\n",
        "\t\t\"\"\"\n",
        "\t\tCalculate entropy, standard deviation, and value range of a given dictionary of data,\n",
        "\t\tconsidering the distribution of the data values.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tdata_dict (dict): A dictionary containing data values.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\tlist: Calculated values for entropy, standard deviation, and value range.\n",
        "\t\t\t\t\t\t\tIf there's only one unique value in the data_dict, entropy is 0,\n",
        "\t\t\t\t\t\t\tand std_dev and value_range are placeholders ('').\n",
        "\t\t\t\t\t\t\tIf the data_dict is empty, all values in the list are placeholders ('').\n",
        "\t\t\"\"\"\n",
        "\t\tnon_zero_data = {key: value for key, value in data_dict.items() if value != 0}\n",
        "\t\tif len(non_zero_data) > 1:\n",
        "\t\t\t\tunique_keys = non_zero_data.keys()\n",
        "\t\t\t\ttotal = 0\n",
        "\t\t\t\tfor value in non_zero_data.values():\n",
        "\t\t\t\t\t\tif isinstance(value, int):\n",
        "\t\t\t\t\t\t\t\ttotal += value\n",
        "\t\t\t\tprob_list = []\n",
        "\t\t\t\tH = 0\n",
        "\n",
        "\t\t\t\tfor key in unique_keys:\n",
        "\t\t\t\t\t\tprob = non_zero_data[key] / total\n",
        "\t\t\t\t\t\tprob_list.append(prob)\n",
        "\t\t\t\t\t\tH += -1 * (prob * math.log2(prob))\n",
        "\n",
        "\t\t\t\tstd = statistics.stdev(prob_list)\n",
        "\t\t\t\tvalue_range = max(prob_list) - min(prob_list)\n",
        "\t\t\t\treturn H, std, value_range\n",
        "\n",
        "\t\tif len(non_zero_data) == 1:\n",
        "\t\t\t\treturn 0, '', ''\n",
        "\n",
        "\t\tif len(non_zero_data) == 0:\n",
        "\t\t\t\treturn '', '', ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoTK9MSFbgJ3"
      },
      "outputs": [],
      "source": [
        "# checks if a given sentence (sent) contains a subject for a given verb index (verb_index).\n",
        "def has_subj(verb_index, sent):\n",
        "\n",
        "\tsubj = ''\n",
        "\n",
        "\tfor tok in sent:\n",
        "\t\tif tok[7] == 'nsubj' and tok[6] == verb_index:\n",
        "\t\t\tsubj = tok\n",
        "\t\t\treturn subj\n",
        "\n",
        "\treturn None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enfEHQL2b2QS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def has_obj(verb_index, sent):\n",
        "\n",
        "\tobj = ''\n",
        "\n",
        "\tfor tok in sent:\n",
        "\t\tif tok[7] == 'obj' and tok[6] == verb_index:\n",
        "\t\t\tobj = tok\n",
        "\t\t\treturn obj\n",
        "\n",
        "\treturn None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqM0X6j1ckLS"
      },
      "outputs": [],
      "source": [
        "# if a given sentence (sent) contains an indirect object (iobj) for a given verb index (verb_index)\n",
        "def has_iobj(verb_index, sent):\n",
        "\n",
        "\tiobj = ''\n",
        "\n",
        "\tfor tok in sent:\n",
        "\t\tif tok[7] == 'iobj' and tok[6] == verb_index:\n",
        "\t\t\tiobj = tok\n",
        "\t\t\treturn iobj\n",
        "\n",
        "\treturn None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwZQfZIwcrqJ"
      },
      "outputs": [],
      "source": [
        "# determines the word order pattern around a given verb index (verb_index) in a given sentence (sent).\n",
        "def word_order(verb_index, sent):\n",
        "\torder = []\n",
        "\n",
        "\tsubj = has_subj(verb_index, sent)\n",
        "\tobj = has_obj(verb_index, sent)\n",
        "\tiobj = has_iobj(verb_index, sent)\n",
        "\tdeprel_map = {'nsubj': 's', 'obj': 'o', 'iobj': 'io', 'VERB': 'v'}\n",
        "\n",
        "\tcore_list = [int(verb_index)]\n",
        "\tif subj is not None:\n",
        "\t\tcore_list.append(int(subj[0]))\n",
        "\tif obj is not None:\n",
        "\t\tcore_list.append(int(obj[0]))\n",
        "\tif iobj is not None:\n",
        "\t\tcore_list.append(int(iobj[0]))\n",
        "\n",
        "\tcore_list.sort()\n",
        "\tfor i in range(len(core_list)):\n",
        "\t\tidx = core_list[i]\n",
        "\t\ttok = sent[idx - 1]\n",
        "\t\tif tok[0] == verb_index:\n",
        "\t\t\torder.append('v')\n",
        "\t\telse:\n",
        "\t\t\torder.append(deprel_map[tok[7]])\n",
        "\n",
        "\treturn order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVkaSfWMe3xT"
      },
      "outputs": [],
      "source": [
        "# identifies subordinate clauses related to a given verb index (verb_index) in a given sentence (sent).\n",
        "def has_surbordinate(verb_index, sent):\n",
        "\n",
        "\tsubordinate = []\n",
        "\n",
        "\tfor tok in sent:\n",
        "\t\tif tok[7] in clauses and tok[6] == verb_index:\n",
        "\t\t\tsubordinate.append(tok)\n",
        "\n",
        "\treturn subordinate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2GuzRYAfIKr"
      },
      "outputs": [],
      "source": [
        "### Get the syntactic dependents of a token ###\n",
        "\n",
        "def dependents(index, sentence):\n",
        "\n",
        "\tdependent = []\n",
        "\n",
        "\tfor tok in sentence:\n",
        "\t\tif tok[6] == index:\n",
        "\t\t\tdependent.append(tok[0])\n",
        "\n",
        "\tif len(dependent) != 0:\n",
        "\t\treturn dependent\n",
        "\n",
        "\treturn None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBqlVuz6f9qO"
      },
      "outputs": [],
      "source": [
        "####### Get the subtree of a syntactic head ######\n",
        "\n",
        "def subtree_generate(index, sentence):\n",
        "\n",
        "\tidxlist = [index]\n",
        "\tmin_idx = len(sentence)\n",
        "\tmax_idx = 0\n",
        "\tdebug_count = 0\n",
        "\twhile len(idxlist) != 0:\n",
        "\n",
        "\t\ti = idxlist.pop()\n",
        "\n",
        "\t\tif int(i) < min_idx:\n",
        "\t\t\tmin_idx = int(i)\n",
        "\n",
        "\t\tif int(i) > max_idx:\n",
        "\t\t\tmax_idx = int(i)\n",
        "\n",
        "\t\ti_d = dependents(i, sentence)\n",
        "\n",
        "\t\tif i_d is not None:\n",
        "\t\t\tfor d in i_d:\n",
        "\t\t\t\t\tidxlist.append(d)\n",
        "\n",
        "\tsubtree = [row[:] for row in sentence[min_idx - 1 : max_idx]]\n",
        "\n",
        "\tsubtree_idx = []\n",
        "\n",
        "\tfor idx in range(min_idx - 1, max_idx):\n",
        "\t\tsubtree_idx.append(int(idx))\n",
        "\n",
        "\tsubtree_idx.sort()\n",
        "\n",
        "\treturn subtree, subtree_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kucvjIlVgBVF"
      },
      "outputs": [],
      "source": [
        "def verbal_inflection(index, sentence):\n",
        "\ttok = sentence[int(index) - 1]\n",
        "\tfeatures = tok[5]\n",
        "\tmood_feature = ''\n",
        "\tnumber_feature = ''\n",
        "\tperson_feature = ''\n",
        "\ttense_feature = ''\n",
        "\tform_feature = ''\n",
        "\tvoice_feature = ''\n",
        "\taspect_feature = ''\n",
        "\n",
        "\tif features != 'None':\n",
        "\t\tverbal_features = features.split('|')\n",
        "\t\tfor feature in verbal_features:\n",
        "\t\t\tif feature.startswith('Mood'):\n",
        "\t\t\t\tmood_feature = feature.split('=')[1]\n",
        "\t\t\tif feature.startswith('Number'):\n",
        "\t\t\t\tnumber_feature = feature.split('=')[1]\n",
        "\t\t\tif feature.startswith('Person'):\n",
        "\t\t\t\tperson_feature = feature.split('=')[1]\n",
        "\t\t\tif feature.startswith('Tense'):\n",
        "\t\t\t\ttense_feature = feature.split('=')[1]\n",
        "\t\t\tif feature.startswith('VerbForm'):\n",
        "\t\t\t\tform_feature = feature.split('=')[1]\n",
        "\t\t\tif feature.startswith('Aspect'):\n",
        "\t\t\t\taspect_feature = feature.split('=')[1]\n",
        "\t\t\tif feature.startswith('Voice'):\n",
        "\t\t\t\tvoice_feature = feature.split('=')[1]\n",
        "\n",
        "\treturn mood_feature, number_feature, person_feature, tense_feature, form_feature, aspect_feature, voice_feature\n",
        "\t# return mood_feature, number_feature, person_feature, tense_feature, form_feature, aspect_feature, voice_feature, evident_feature, polarity_feature, polite_feature, clusivity_feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Z1vBc9grRR"
      },
      "outputs": [],
      "source": [
        "# calculates the ratio of each value in a given dictionary to the total sum of all values.\n",
        "def dictionary_ratio(dictionary):\n",
        "\n",
        "\ttotal = 0\n",
        "\tfor k, v in dictionary.items():\n",
        "\t\ttotal += v\n",
        "\n",
        "\tif total != 0:\n",
        "\t\tfor k, v in dictionary.items():\n",
        "\t\t\tdictionary[k] = v / total\n",
        "\n",
        "\treturn dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzOxayjMg4Z1"
      },
      "outputs": [],
      "source": [
        "# performs an analysis on a graph g representing a dependency tree.\n",
        "def analyze(g):\n",
        "\tnp_words = set()\n",
        "\tdoms = {}\n",
        "\n",
        "\t# for each node in g, get list of dominating nodes\n",
        "\tfor node in g:\n",
        "\t\tdoms[node] = nx.shortest_path(g, source = 0, target = node)\n",
        "\n",
        "\t# for each edge (u,v) in g, determine whether all intervening words (ids) are dominated by u\n",
        "\tfor edge in g.edges():\n",
        "\t\tu = edge[0]\n",
        "\t\tv = edge[1]\n",
        "\t\ti = 1\n",
        "\t\tif u > v:\n",
        "\t\t\ti = -1\n",
        "\t\tfor n in range(u+i,v,i):\n",
        "\t\t\tif u not in doms[n]:\n",
        "\t\t\t\tnp_words.add(n)\n",
        "\n",
        "\tn = len(np_words)\n",
        "\n",
        "\t# for calculating dependency tree depth\n",
        "\tlen_list = []\n",
        "\n",
        "\tfor k, v in doms.items():\n",
        "\t\tlen_list.append(len(v))\n",
        "\n",
        "\tlen_list = list(set(len_list))\n",
        "\ttemp_max_depth = max(len_list)\n",
        "\n",
        "\tif temp_max_depth == 2:\n",
        "\t\tmax_depth = 1\n",
        "\telse:\n",
        "\t\tmax_depth = temp_max_depth - 2\n",
        "\n",
        "\t# return whether this tree has a non-projective dependency and number of non-projective words (ids)\n",
        "\treturn int(n > 0), n, max_depth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usvnMqDJtjOA"
      },
      "outputs": [],
      "source": [
        "def has_cycle(sent):\n",
        "    graph = nx.DiGraph()\n",
        "\n",
        "    for tok in sent:\n",
        "        graph.add_edge(int(tok[6]), int(tok[0]))\n",
        "\n",
        "    try:\n",
        "        cycle = nx.find_cycle(graph, orientation='original')\n",
        "        return True\n",
        "    except nx.NetworkXNoCycle:\n",
        "        return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oANM2Y1PYT-9"
      },
      "outputs": [],
      "source": [
        "def update_dictionary(dictionary, key, increment=1, new_item=True):\n",
        "    if key in dictionary:\n",
        "        dictionary[key] += increment\n",
        "    else:\n",
        "        if new_item == True:\n",
        "            dictionary[key] = increment\n",
        "\n",
        "\n",
        "def morphological_feature_analysis(file_path):\n",
        "\n",
        "\t\tnum_sent, num_word, num_word_type, num_function_word, num_function_word_type = 0, 0, 0, 0, 0\n",
        "\t\tnum_lexical_density, num_lemma_type, num_pos_type, ttr_word, ttr_lemma = 0, 0, 0, 0, 0\n",
        "\t\tave_sent_len, ave_word_len = 0, 0\n",
        "\n",
        "\t\tword_list, word_len_list, lemma_list, lemma_len_list = [], [], [], []\n",
        "\t\tdeplen_list = []\n",
        "\t\tverb_word_dict, verb_lemma_dict = {}, {}\n",
        "\t\taux_word_dict, aux_lemma_dict = {}, {}\n",
        "\t\tfunction_word_dict, function_lemma_dict = {}, {}\n",
        "\t\tpos_dict = {}\n",
        "\t\tdeprel_dict = {}\n",
        "\t\theadedness_dict = {'final': 0, 'initial': 0}\n",
        "\t\tsubordinate_deprel_dict, all_subordinate_order_dict = {}, {}\n",
        "\n",
        "\t\tclause_len_list = []\n",
        "\t\tall_word_order_dict = {}\n",
        "\n",
        "\t\tword_order_dict = {'s_v': 0, 'v_s': 0, 'v_o': 0, 'o_v': 0}\n",
        "\t\tsubordinate_order_dictionary = {'subordinate_head_initial': 0, 'subordinate_head_final': 0}\n",
        "\t\tvalency_dict = {'intransitive': 0, 'transitive': 0, 'ditransitive': 0, 'v_io': 0}\n",
        "\n",
        "\t\tdef dictionary_initialization(feature_name, features_category_list):\n",
        "\t\t\t\tdictionary = {}\n",
        "\t\t\t\tfor feature in features_category_list:\n",
        "\t\t\t\t\t\tdictionary[feature_name + feature] = 0\n",
        "\t\t\t\treturn dictionary\n",
        "\n",
        "\t\tverb_mood_dict = dictionary_initialization('verb_mood_', verb_mood_features)\n",
        "\t\tverb_number_dict = dictionary_initialization('verb_number_', verb_number_features)\n",
        "\t\tverb_tense_dict = dictionary_initialization('verb_tense_', verb_tense_features)\n",
        "\t\tverb_aspect_dict = dictionary_initialization('verb_aspect_', verb_aspect_features)\n",
        "\t\tverb_voice_dict = dictionary_initialization('verb_voice_', verb_voice_features)\n",
        "\t\tverb_form_dict = dictionary_initialization('verb_form_', verb_form_features)\n",
        "\t\tverb_person_dict = dictionary_initialization('verb_person_', verb_person_features)\n",
        "\n",
        "\t\taux_mood_dict = dictionary_initialization('aux_mood_', verb_mood_features)\n",
        "\t\taux_number_dict = dictionary_initialization('aux_number_', verb_number_features)\n",
        "\t\taux_tense_dict = dictionary_initialization('aux_tense_', verb_tense_features)\n",
        "\t\taux_voice_dict = dictionary_initialization('aux_voice_', verb_voice_features)\n",
        "\t\taux_form_dict = dictionary_initialization('aux_form_', verb_form_features)\n",
        "\t\taux_person_dict = dictionary_initialization('aux_person_', verb_person_features)\n",
        "\n",
        "\t\tnon_projective_sent, non_projective_word, total_depth = 0, 0, 0\n",
        "\t\tsubordinate_non_projective_sent, subordinate_non_projective_word, subordinate_total_depth = 0, 0, 0\n",
        "\n",
        "\t\tlg = ''\n",
        "\n",
        "\t\tsentences = read_conllu_file(file_path)\n",
        "\n",
        "\t\tfor sent in sentences:\n",
        "\t\t\t\tif len(sent) >= 5:\n",
        "\t\t\t\t\t\tif lg != '' and lg != sent[0][-1]:\n",
        "\t\t\t\t\t\t\t\traise ValueError(f'Inconsistant native language {lg} != {sent[0][-1]} in file: {file_path}')\n",
        "\t\t\t\t\t\telif lg == '':\n",
        "\t\t\t\t\t\t\t\tlg = sent[0][-1]\n",
        "\n",
        "\t\t\t\t\t\tG = has_cycle(sent)\n",
        "\t\t\t\t\t\tif G == True:\n",
        "\t\t\t\t\t\t\t\traise ValueError('cyclic dependency tree')\n",
        "\t\t\t\t\t\tns, nw, max_depth = 0, 0, 0\n",
        "\t\t\t\t\t\tfor tok in sent:\n",
        "\t\t\t\t\t\t\t\tG.add_edge(int(tok[6]), int(tok[0])) # 6: head; 0: word index\n",
        "\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\t\tns, nw, max_depth = analyze(G)\n",
        "\t\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t\t\tprint(len(sent))\n",
        "\t\t\t\t\t\t\t\tprint(file_name, ' '.join(tok[1] for tok in sent), 'cannot analyze!!')\n",
        "\t\t\t\t\t\tnon_projective_sent += ns\n",
        "\t\t\t\t\t\tnon_projective_word += nw\n",
        "\t\t\t\t\t\ttotal_depth += max_depth\n",
        "\n",
        "\t\t\t\t\t\tnum_sent += 1\n",
        "\t\t\t\t\t\tnum_word += len(sent)\n",
        "\n",
        "\t\t\t\t\t\tfor tok in sent:\n",
        "\t\t\t\t\t\t\t\tword_list.append(tok[1])\n",
        "\t\t\t\t\t\t\t\tword_len_list.append(len(tok[1]))\n",
        "\t\t\t\t\t\t\t\tlemma_list.append(tok[2])\n",
        "\t\t\t\t\t\t\t\tlemma_len_list.append(len(tok[2]))\n",
        "\t\t\t\t\t\t\t\tupdate_dictionary(pos_dict, tok[3])\n",
        "\t\t\t\t\t\t\t\tupdate_dictionary(deprel_dict, tok[7])\n",
        "\n",
        "\t\t\t\t\t\t\t\tif int(tok[6]) != 0:\n",
        "\t\t\t\t\t\t\t\t\t\tdeplen_list.append(abs(int(tok[6]) - int(tok[0])))\n",
        "\t\t\t\t\t\t\t\t\t\tif int(tok[6]) > int(tok[0]):\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tupdate_dictionary(headedness_dict, 'final')\n",
        "\t\t\t\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tupdate_dictionary(headedness_dict, 'initial')\n",
        "\t\t\t\t\t\t\t\t### collecting funcational words using pos tags\n",
        "\t\t\t\t\t\t\t\tif tok[3] in function_POS:\n",
        "\t\t\t\t\t\t\t\t\t\tnum_function_word += 1\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(function_word_dict, tok[1])\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(function_lemma_dict, tok[2])\n",
        "\t\t\t\t\t\t\t\t### collecting verbal inflectional features\n",
        "\t\t\t\t\t\t\t\tif tok[3] == 'VERB':\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_word_dict, tok[1])\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_lemma_dict, tok[2])\n",
        "\t\t\t\t\t\t\t\t\t\tmood_feature, number_feature, person_feature, tense_feature, form_feature, aspect_feature, voice_feature = verbal_inflection(tok[0], sent)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_mood_dict, 'verb_mood_' + mood_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_number_dict, 'verb_number_' + number_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_person_dict, 'verb_person_' + person_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_tense_dict, 'verb_tense_' + tense_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_form_dict, 'verb_form_' + form_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_aspect_dict, 'verb_aspect_' + aspect_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(verb_voice_dict, 'verb_voice_' + voice_feature, new_item=False)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t### collecting valency features\n",
        "\t\t\t\t\t\t\t\t\t\torder = word_order(tok[0], sent)\n",
        "\t\t\t\t\t\t\t\t\t\tif 'o' in order and 'io' in order:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tvalency_dict['ditransitive'] += 1\n",
        "\t\t\t\t\t\t\t\t\t\tif 'o' in order and 'io' not in order:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tvalency_dict['transitive'] += 1\n",
        "\t\t\t\t\t\t\t\t\t\tif 'o' not in order and 'io' in order:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tvalency_dict['v_io'] += 1\n",
        "\t\t\t\t\t\t\t\t\t\tif 'o' not in order and 'io' not in order:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tvalency_dict['intransitive'] += 1\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tif 's' in order and 'v' in order:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tif order.index('s') < order.index('v'):\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_order_dict['s_v'] += 1\n",
        "\t\t\t\t\t\t\t\t\t\t\t\telif order.index('s') > order.index('v'):\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_order_dict['v_s'] += 1\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tif 'o' in order and 'v' in order:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tif order.index('o') < order.index('v'):\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_order_dict['o_v'] += 1\n",
        "\t\t\t\t\t\t\t\t\t\t\t\telif order.index('o') > order.index('v'):\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_order_dict['v_o'] += 1\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(all_word_order_dict, '_'.join(w for w in order))\n",
        "\n",
        "\t\t\t\t\t\t\t\tif tok[3] == 'AUX':\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_word_dict, tok[1])\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_lemma_dict, tok[2])\n",
        "\t\t\t\t\t\t\t\t\t\tmood_feature, number_feature, person_feature, tense_feature, form_feature, aspect_feature, voice_feature = verbal_inflection(tok[0], sent)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_mood_dict, 'aux_mood_' + mood_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_number_dict, 'aux_number_' + number_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_person_dict, 'aux_person_' + person_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_tense_dict, 'aux_tense_' + tense_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_form_dict, 'aux_form_' + form_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(aux_voice_dict, 'aux_voice_' + voice_feature, new_item=False)\n",
        "\t\t\t\t\t\t\t\tif tok[7] in clauses:\n",
        "\t\t\t\t\t\t\t\t\t\tupdate_dictionary(subordinate_deprel_dict, tok[7])\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\thead = sent[int(tok[6]) - 1]\n",
        "\t\t\t\t\t\t\t\t\t\tif int(tok[6]) > int(tok[0]):\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tupdate_dictionary(all_subordinate_order_dict, tok[7] + '_' + head[7])\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tsubordinate_order_dictionary['subordinate_head_final'] += 1\n",
        "\t\t\t\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tupdate_dictionary(all_subordinate_order_dict, head[7] + '_' + tok[7])\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tsubordinate_order_dictionary['subordinate_head_initial'] += 1\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tsubordinate_ns, subordinate_nw, subordinate_max_depth = 0, 0, 0\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tclause_subtree, clause_subtree_idx = subtree_generate(tok[0], sent)\n",
        "\t\t\t\t\t\t\t\t\t\t##### re-indexing the clause dependency index\n",
        "\t\t\t\t\t\t\t\t\t\troot = int(clause_subtree[0][0])-1\n",
        "\t\t\t\t\t\t\t\t\t\tlim = len(clause_subtree)\n",
        "\t\t\t\t\t\t\t\t\t\t# print(root)\n",
        "\t\t\t\t\t\t\t\t\t\tfor i in range(len(clause_subtree)):\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tclause_subtree[i][0] = str(int(clause_subtree[i][0]) - root)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tclause_subtree[i][6] = str(max(0, int(clause_subtree[i][6]) - root))\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tif int(clause_subtree[i][6]) > lim:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tclause_subtree[i][6] = '0'\n",
        "\t\t\t\t\t\t\t\t\t\t#####\n",
        "\t\t\t\t\t\t\t\t\t\tclause_len_list.append(len(clause_subtree))\n",
        "\t\t\t\t\t\t\t\t\t\tsubordinate_G = nx.DiGraph()\n",
        "\t\t\t\t\t\t\t\t\t\tfor z in clause_subtree:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tsubordinate_G.add_edge(int(z[6]), int(z[0]))\n",
        "\t\t\t\t\t\t\t\t\t\tsubordinate_ns, subordinate_nw, subordinate_max_depth = 0, 0, 0\n",
        "\t\t\t\t\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tsubordinate_ns, subordinate_nw, subordinate_max_depth = analyze(subordinate_G)\n",
        "\t\t\t\t\t\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tprint(file_name, ' '.join(tok[1] for tok in sent), 'cannot analyze clause')\n",
        "\t\t\t\t\t\t\t\t\t\tsubordinate_non_projective_sent += subordinate_ns\n",
        "\t\t\t\t\t\t\t\t\t\tsubordinate_non_projective_word += subordinate_nw\n",
        "\t\t\t\t\t\t\t\t\t\tsubordinate_total_depth += subordinate_max_depth\n",
        "\n",
        "\t\tdef sum_dictionary_values(dictionary):\n",
        "\t\t\t\ttotal = 0\n",
        "\t\t\t\tfor value in dictionary.values():\n",
        "\t\t\t\t\t\tif isinstance(value, int):\n",
        "\t\t\t\t\t\t\t\ttotal += value\n",
        "\t\t\t\treturn total\n",
        "\n",
        "\t\tif num_word == 0:\n",
        "\t\t\t\treturn None, None\n",
        "\t\tnum_word_type = len(set(word_list))\n",
        "\t\tfunction_word_type = len(function_word_dict) / num_word_type\n",
        "\t\tcontent_word = num_word - num_function_word\n",
        "\t\tcontent_word_type = 1 - function_word_type\n",
        "\t\tlexical_density = content_word / num_word\n",
        "\t\tfunction_word_H, function_word_std, function_word_range = distribution(function_word_dict)\n",
        "\t\tfunction_lemma_H, function_lemma_std, function_lemma_range = distribution(function_lemma_dict)\n",
        "\t\tnum_lemma_type = len(set(lemma_list))\n",
        "\t\tfunction_lemma_type = len(function_lemma_dict) / num_lemma_type\n",
        "\t\tcontent_lemma_type = 1 - function_lemma_type\n",
        "\t\tnum_pos_type = len(pos_dict) / num_word\n",
        "\t\tpos_H, pos_std, pos_range = distribution(pos_dict)\n",
        "\t\tttr_word = num_word_type / num_word\n",
        "\t\tttr_lemma = num_lemma_type / num_word\n",
        "\t\tave_sent_len = num_word / num_sent\n",
        "\t\tave_word_len = sum(word_len_list) / num_word\n",
        "\t\tave_lemma_len = sum(lemma_len_list) / num_word\n",
        "\n",
        "\t\tverb_word_H, verb_word_std, verb_word_range = distribution(verb_word_dict)\n",
        "\t\tverb_lemma_H, verb_lemma_std, verb_lemma_range = distribution(verb_lemma_dict)\n",
        "\t\tverb_mood_H, verb_mood_std, verb_mood_range = distribution(verb_mood_dict)\n",
        "\t\tverb_number_H, verb_number_std, verb_number_range = distribution(verb_number_dict)\n",
        "\t\tverb_person_H, verb_person_std, verb_person_range = distribution(verb_person_dict)\n",
        "\t\tverb_tense_H, verb_tense_std, verb_tense_range = distribution(verb_tense_dict)\n",
        "\t\tverb_form_H, verb_form_std, verb_form_range = distribution(verb_form_dict)\n",
        "\t\tverb_aspect_H, verb_aspect_std, verb_aspect_range = distribution(verb_aspect_dict)\n",
        "\n",
        "\t\tverb_valency_H, verb_valency_std, verb_valency_range = distribution(valency_dict)\n",
        "\n",
        "\t\taux_word_H, aux_word_std, aux_word_range = distribution(aux_word_dict)\n",
        "\t\taux_lemma_H, aux_lemma_std, aux_lemma_range = distribution(aux_lemma_dict)\n",
        "\t\taux_mood_H, aux_mood_std, aux_mood_range = distribution(aux_mood_dict)\n",
        "\t\taux_number_H, aux_number_std, aux_number_range = distribution(aux_number_dict)\n",
        "\t\taux_person_H, aux_person_std, aux_person_range = distribution(aux_person_dict)\n",
        "\t\taux_tense_H, aux_tense_std, aux_tense_range = distribution(aux_tense_dict)\n",
        "\t\taux_form_H, aux_form_std, aux_form_range = distribution(aux_form_dict)\n",
        "\n",
        "\t\tdeprel_H , deprel_std, deprel_range = distribution(deprel_dict)\n",
        "\n",
        "\t\tsubordinate_deprel_H , subordinate_deprel_std, subordinate_deprel_range = distribution(subordinate_deprel_dict)\n",
        "\t\tsubordinate_order_H, subordinate_order_std, subordinate_order_range = distribution(all_subordinate_order_dict)\n",
        "\n",
        "\t\thead_finality = headedness_dict['final'] / (headedness_dict['final'] + headedness_dict['initial'])\n",
        "\n",
        "\t\tword_order_H, word_order_std, word_order_range = distribution(all_word_order_dict)\n",
        "\n",
        "\t\tave_dep_len = sum(deplen_list) / len(deplen_list)\n",
        "\t\tave_clause_len = 0\n",
        "\t\tif len(clause_len_list) > 0:\n",
        "\t\t\t\tave_clause_len = sum(clause_len_list) / len(clause_len_list)\n",
        "\t\telse:\n",
        "\t\t\t\tave_clause_len = 0\n",
        "\t\tnon_projective_sent_ratio = non_projective_sent / num_sent\n",
        "\t\tnon_projective_word_ratio = non_projective_word / num_word\n",
        "\t\tave_tree_depth = total_depth / num_sent\n",
        "\n",
        "\t\tsubordinate_non_projective_sent_ratio = 0\n",
        "\t\tif len(clause_len_list) > 0:\n",
        "\t\t\t\tsubordinate_non_projective_sent_ratio = subordinate_non_projective_sent / len(clause_len_list)\n",
        "\t\telse:\n",
        "\t\t\t\tsubordinate_non_projective_sent_ratio = 0\n",
        "\t\tif sum(clause_len_list) > 0:\n",
        "\t\t\t\tsubordinate_non_projective_word_ratio = subordinate_non_projective_word / sum(clause_len_list)\n",
        "\t\telse:\n",
        "\t\t\t\tsubordinate_non_projective_word_ratio = 0\n",
        "\t\tif len(clause_len_list) > 0:\n",
        "\t\t\t\tsubordinate_ave_tree_depth = subordinate_total_depth / len(clause_len_list)\n",
        "\t\telse:\n",
        "\t\t\t\tsubordinate_ave_tree_depth = 0\n",
        "\n",
        "\t\tverb_ratio = sum_dictionary_values(verb_word_dict) / num_sent\n",
        "\n",
        "\t\tverb_features = [verb_mood_dict, verb_number_dict, verb_person_dict, verb_tense_dict, verb_form_dict, verb_aspect_dict, verb_voice_dict]\n",
        "\t\taux_features = [aux_mood_dict, aux_number_dict, aux_person_dict, aux_tense_dict, aux_form_dict, aux_voice_dict]\n",
        "\t\tall_info = [ave_sent_len, ttr_word, ave_word_len,\n",
        "\t\t\t\t\t\tfunction_word_type, function_word_H, function_word_std, function_word_range,\n",
        "\t\t\t\t\t\tlexical_density, ttr_lemma, ave_lemma_len, function_lemma_type,\n",
        "\t\t\t\t\t\tfunction_lemma_H, function_lemma_std, function_lemma_range,\n",
        "\t\t\t\t\t\tnum_pos_type, pos_H, pos_std, pos_range,\n",
        "\t\t\t\t\t\tverb_word_H, verb_word_std, verb_word_range,\n",
        "\t\t\t\t\t\tverb_lemma_H, verb_lemma_std, verb_lemma_range,\n",
        "\t\t\t\t\t\tverb_mood_H, verb_mood_std, verb_mood_range,\n",
        "\t\t\t\t\t\tverb_number_H, verb_number_std, verb_number_range,\n",
        "\t\t\t\t\t\tverb_person_H, verb_person_std, verb_person_range,\n",
        "\t\t\t\t\t\tverb_tense_H, verb_tense_std, verb_tense_range,\n",
        "\t\t\t\t\t\tverb_form_H, verb_form_std, verb_form_range,\n",
        "\t\t\t\t\t\tverb_valency_H, verb_valency_std, verb_valency_range,\n",
        "\t\t\t\t\t\tverb_aspect_H, verb_aspect_std, verb_aspect_range,\n",
        "\t\t\t\t\t\taux_word_H, aux_word_std, aux_word_range,\n",
        "\t\t\t\t\t\taux_lemma_H, aux_lemma_std, aux_lemma_range,\n",
        "\t\t\t\t\t\taux_mood_H, aux_mood_std, aux_mood_range,\n",
        "\t\t\t\t\t\taux_number_H, aux_number_std, aux_number_range,\n",
        "\t\t\t\t\t\taux_person_H, aux_person_std, aux_person_range,\n",
        "\t\t\t\t\t\taux_tense_H, aux_tense_std, aux_tense_range,\n",
        "\t\t\t\t\t\taux_form_H, aux_form_std, aux_form_range,\n",
        "\t\t\t\t\t\tdeprel_H , deprel_std, deprel_range,\n",
        "\t\t\t\t\t\tsubordinate_deprel_H, subordinate_deprel_std, subordinate_deprel_range,\n",
        "\t\t\t\t\t\tsubordinate_order_H, subordinate_order_std, subordinate_order_range,\n",
        "\t\t\t\t\t\thead_finality, verb_valency_H, verb_valency_std, verb_valency_range,\n",
        "\t\t\t\t\t\tword_order_H, word_order_std, word_order_range, ave_dep_len, ave_clause_len,\n",
        "\t\t\t\t\t\tnon_projective_sent_ratio, non_projective_word_ratio, ave_tree_depth,\n",
        "\t\t\t\t\t\tsubordinate_non_projective_sent, subordinate_non_projective_word_ratio, subordinate_ave_tree_depth,\n",
        "\t\t\t\t\t\tverb_ratio]\n",
        "\n",
        "\t\tword_order_dictionary = dictionary_ratio(word_order_dict)\n",
        "\t\tfor k, v in word_order_dictionary.items():\n",
        "\t\t\t\tall_info.append(v)\n",
        "\n",
        "\t\tsubordinate_order_dictionary = dictionary_ratio(subordinate_order_dictionary)\n",
        "\t\tfor k, v in subordinate_order_dictionary.items():\n",
        "\t\t\t\tall_info.append(v)\n",
        "\n",
        "\t\tfor feature_dictionary in verb_features:\n",
        "\t\t\t\tfeature_dictionary = dictionary_ratio(feature_dictionary)\n",
        "\t\t\t\tfor k, v in feature_dictionary.items():\n",
        "\t\t\t\t\t\tall_info.append(v)\n",
        "\n",
        "\t\tfor feature_dictionary in aux_features:\n",
        "\t\t\t\tfeature_dictionary = dictionary_ratio(feature_dictionary)\n",
        "\t\t\t\tfor k, v in feature_dictionary.items():\n",
        "\t\t\t\t\t\tall_info.append(v)\n",
        "\n",
        "\n",
        "\t\treturn lg, all_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I4v5W4GJYdL"
      },
      "source": [
        "# doc2vec topic embedding (only for English)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkNW-oDnP1rv",
        "outputId": "0a90ff9f-e31d-46a6-9735-01a4f54aa74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-31 18:26:44--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2023-08-31 18:26:44--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                     0%[                    ]       0  --.-KB/s               Installing PySpark 3.3.0 and Spark NLP 5.0.0\n",
            "setup Colab for PySpark 3.3.0 and Spark NLP 5.0.0\n",
            "Upgrading libcudnn8 to 8.1.0 for GPU\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-31 18:26:44 (87.9 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.7/498.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.3.0 -s 5.0.0 -g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqNQLgIoP9g3"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3dsIGmAJc1G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "462876b2-8ed8-4bc9-efe4-29e46bb57216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version 5.0.0\n",
            "Apache Spark version: 3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79608ff0afb0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a088e20d58c8:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "spark = sparknlp.start()# for GPU training >> sparknlp.start(gpu = True)\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euCg2mOPJ868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e98b08b-4c0f-425f-b974-9671bc6250ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopwords_en download started this may take some time.\n",
            "Approximate size to download 2.9 KB\n",
            "[OK!]\n",
            "doc2vec_gigaword_wiki_300 download started this may take some time.\n",
            "Approximate size to download 312.3 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "token = Tokenizer()\\\n",
        ".setInputCols(\"document\")\\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "norm = Normalizer()\\\n",
        ".setInputCols([\"token\"])\\\n",
        ".setOutputCol(\"normalized\")\\\n",
        ".setLowercase(True)\n",
        "\n",
        "stops = StopWordsCleaner.pretrained()\\\n",
        ".setInputCols(\"normalized\")\\\n",
        ".setOutputCol(\"cleanedToken\")\n",
        "\n",
        "doc2Vec = Doc2VecModel.pretrained(\"doc2vec_gigaword_wiki_300\", \"en\")\\\n",
        ".setInputCols(\"cleanedToken\")\\\n",
        ".setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "    stages=[\n",
        "      document,\n",
        "      token,\n",
        "      norm,\n",
        "      stops,\n",
        "      doc2Vec\n",
        "      ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n521Kya2J_Tx"
      },
      "outputs": [],
      "source": [
        "raw_text_input_extraction_path = f'/content/unzipped_inputs/raw_text/{corpus_name}'\n",
        "shutil.unpack_archive(raw_text_input_directory, raw_text_input_extraction_path, 'zip')\n",
        "\n",
        "\n",
        "def extract_topic_embedding(file_handle):\n",
        "    file_name = file_handle.replace('.pred', '.txt')\n",
        "    file_exist = False\n",
        "    if nativeness == 'Native':\n",
        "        file_path = os.path.join(raw_text_input_extraction_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            file_exist = True\n",
        "    elif nativeness == 'Learner':\n",
        "        for dir in os.listdir(raw_text_input_extraction_path):\n",
        "            language_path = os.path.join(raw_text_input_extraction_path, dir)\n",
        "            file_path = os.path.join(language_path, file_name)\n",
        "            if os.path.exists(file_path):\n",
        "                file_exist = True\n",
        "                break\n",
        "\n",
        "    if file_exist:\n",
        "        with open(file_path, 'r') as text_file:\n",
        "            file_content = [text_file.read()]\n",
        "        df = spark.createDataFrame(file_content, StringType()).toDF(\"text\")\n",
        "        result = nlpPipeline.fit(df).transform(df)\n",
        "        new_df = result.select(F.explode(F.arrays_zip(result.document.result,\n",
        "                                             result.sentence_embeddings.embeddings)).alias(\"cols\")) \\\n",
        "              .select(F.expr(\"cols['0']\").alias(\"document\"),\n",
        "                      F.expr(\"cols['1']\").alias(\"embeddings\"))\n",
        "\n",
        "        # Collect the rows from the DataFrame as a list of Row objects\n",
        "        rows = new_df.collect()\n",
        "        for row in rows:\n",
        "            embeddings = row.embeddings # only 1 row\n",
        "        return embeddings\n",
        "    else:\n",
        "        raise ValueError(f'Input file: {file_path} not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu5zFZi0gNgy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxghr09RgSNE"
      },
      "source": [
        "# lang2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u308KFgmHuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b10e12-8956-4545-ae34-4e413af9af4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.4/167.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: iso-639\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=168842 sha256=6715ae64ff6a1ada28ce61da4ae13a4fbacb3708d7d49e352469262c4b2c66ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/78/cc/5478ca3b1c3f602eae6f8cdbd78f909c0a0bfa0bbcb5c7771f\n",
            "Successfully built iso-639\n",
            "Installing collected packages: iso-639\n",
            "Successfully installed iso-639-0.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install iso-639\n",
        "from iso639 import languages\n",
        "\n",
        "def loading_packages():\n",
        "    !git clone https://github.com/antonisa/lang2vec\n",
        "    directory = '/content/lang2vec/'\n",
        "    os.chdir(directory)\n",
        "    !python3 /content/lang2vec/setup.py install\n",
        "    import lang2vec.lang2vec as l2v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u-92n188AVw"
      },
      "outputs": [],
      "source": [
        "# delete unnecessary temporary directory\n",
        "def delete_directory(directory):\n",
        "    for root, dirs, files in os.walk(directory, topdown=False):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            os.remove(file_path)\n",
        "        for dir in dirs:\n",
        "            dir_path = os.path.join(root, dir)\n",
        "            os.rmdir(dir_path)\n",
        "    os.rmdir(directory)\n",
        "\n",
        "\n",
        "def unzip_read(zip_file):\n",
        "    # unzip input files\n",
        "    l1s = {}\n",
        "    input_extraction_path = f'/content/temp_unzipped'\n",
        "    shutil.unpack_archive(zip_file, input_extraction_path, 'zip')\n",
        "    for dir in os.listdir(input_extraction_path):\n",
        "        if os.path.isdir(os.path.join(input_extraction_path, dir)):\n",
        "            file_num = len(os.listdir(os.path.join(input_extraction_path, dir)))\n",
        "            if dir in l1s:\n",
        "                l1s[dir] += file_num\n",
        "            else:\n",
        "                l1s[dir] = file_num\n",
        "\n",
        "    delete_directory(input_extraction_path)\n",
        "    return l1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa_uCsLhj-Fd"
      },
      "outputs": [],
      "source": [
        "def get_lg_code(lg_name):\n",
        "    try:\n",
        "        lg = languages.get(name=lg_name)\n",
        "    except KeyError:\n",
        "        return f'Not found ISO-639-3 language code for {lg_name}'\n",
        "    return lg.part3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtZ7Nw5JnzL7"
      },
      "outputs": [],
      "source": [
        "L2_code = get_lg_code(language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snxUWfHdlGMg"
      },
      "outputs": [],
      "source": [
        "first_language_filtered = ['Unknown', 'Filipino', 'Other', 'Bosnian-croatian-serbian', 'Pakistan', 'Malay', 'Philippines', 'Taiwanese']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYxwFFX7lGzF"
      },
      "outputs": [],
      "source": [
        "def language_distance(first_language):\n",
        "    if first_language not in first_language_filtered:\n",
        "        if first_language == \"Chinese-Mandarin\":\n",
        "            first_language = 'Mandarin Chinese'\n",
        "        if first_language == \"Chinese-Cantonese\":\n",
        "            first_language = 'Yue Chinese'\n",
        "        if first_language == 'Russian, Ukrainian':\n",
        "            first_language = 'Ukrainian'\n",
        "        if first_language == 'Aromanian (Vlach)':\n",
        "            first_language = 'Aromanian'\n",
        "        if first_language == 'Greek':\n",
        "            first_language = 'Modern Greek (1453-)'\n",
        "        if first_language == 'Farsi':\n",
        "            first_language = 'Persian'\n",
        "        if first_language == 'Swahili':\n",
        "            first_language = 'Swahili (macrolanguage)'\n",
        "        L1_code = get_lg_code(first_language)\n",
        "        if L1_code.startswith('Not found ISO-639-3 language code for '):\n",
        "            print(f'{first_language}: {L1_code}, essays with this L1 were skipped')\n",
        "        else:\n",
        "            featural_score = l2v.distance('featural', L1_code, L2_code)\n",
        "            phonological_score = l2v.distance('phonological', L1_code, L2_code)\n",
        "            syntactic_score = l2v.distance('syntactic', L1_code, L2_code)\n",
        "\n",
        "            return [featural_score, phonological_score, syntactic_score]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k5EZAI26xkk"
      },
      "outputs": [],
      "source": [
        "pre_computed_lang2vec = f'/content/drive/MyDrive/cvae_project/codes/lang2vec/{language}.csv'\n",
        "if not os.path.exists(pre_computed_lang2vec):\n",
        "    loading_packages()\n",
        "    print(f'No pre-computed lang2vec values prepared for {language}\\nPreparing a look up csv file to speed up the codes...')\n",
        "    corpus_dir = f'/content/drive/MyDrive/cvae_project/1_extract_txt_format/{language}/Learner/'\n",
        "\n",
        "    corpus_l1s = {}\n",
        "    for zip_file in os.listdir(corpus_dir):\n",
        "        if zip_file.endswith('.zip'):\n",
        "            zip_path = os.path.join(corpus_dir, zip_file)\n",
        "            l1s = unzip_read(zip_path)\n",
        "            corpus_l1s[zip_file.replace('.zip', '')] = l1s\n",
        "\n",
        "    language_list = []\n",
        "    for zip_file, l1s in corpus_l1s.items():\n",
        "        print(f'{zip_file}: {l1s}')\n",
        "        for l1 in l1s:\n",
        "            if l1 not in language_list:\n",
        "                language_list.append(l1)\n",
        "\n",
        "    with open(pre_computed_lang2vec, mode='w', newline='') as csv_file:\n",
        "        fieldnames = ['L1 languages', 'featural scores', 'phonological scores', 'syntactic scores']\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(fieldnames)\n",
        "\n",
        "        for lg in tqdm(language_list):\n",
        "            new_data = [lg, 0, 0, 0]\n",
        "            scores = language_distance(lg)\n",
        "            if scores != None:\n",
        "                new_data[1] = scores[0]\n",
        "                new_data[2] = scores[1]\n",
        "                new_data[3] = scores[2]\n",
        "\n",
        "                writer.writerow(new_data)\n",
        "    print(\"\\nCSV file created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lAHpPElO3sb"
      },
      "outputs": [],
      "source": [
        "language_distance_scores = {}\n",
        "\n",
        "with open(pre_computed_lang2vec, mode='r') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        language = row['L1 languages']\n",
        "        scores = [float(row['featural scores']), float(row['phonological scores']), float(row['syntactic scores'])]\n",
        "        language_distance_scores[language] = scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-6zAeZKSSxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb97b818-5a23-4da2-e24d-8dda9b1f953c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Arabic': [0.6, 0.5687, 0.57],\n",
              " 'Tswana': [1.0, 0.5687, 1.0],\n",
              " 'Greek': [0.5, 0.195, 0.52],\n",
              " 'Japanese': [0.6, 0.5032, 0.66],\n",
              " 'Polish': [0.5, 0.2804, 0.59],\n",
              " 'Chinese': [0.6, 0.5687, 0.57],\n",
              " 'Persian': [0.6, 0.5687, 0.57],\n",
              " 'Swedish': [0.5, 0.5687, 0.42],\n",
              " 'Russian': [0.5, 0.2804, 0.49],\n",
              " 'Macedonian': [0.6, 0.5687, 0.57],\n",
              " 'Korean': [0.5, 0.4638, 0.62],\n",
              " 'Czech': [0.6, 0.5687, 0.66],\n",
              " 'Aromanian (Vlach)': [0.6, 0.5687, 0.57],\n",
              " 'Dutch': [0.5, 0.5687, 0.49],\n",
              " 'Spanish': [0.5, 0.3433, 0.4],\n",
              " 'Punjabi': [0.6, 0.5687, 0.65],\n",
              " 'Norwegian': [0.8, 0.5687, 0.59],\n",
              " 'Bulgarian': [0.5, 0.2804, 0.48],\n",
              " 'Italian': [0.5, 0.5687, 0.51],\n",
              " 'Hungarian': [0.5, 0.3433, 0.6],\n",
              " 'Chinese-Mandarin': [0.6, 0.39, 0.55],\n",
              " 'Lithuanian': [0.5, 0.3498, 0.68],\n",
              " 'Urdu': [0.6, 0.5687, 0.67],\n",
              " 'German': [0.4, 0.3277, 0.42],\n",
              " 'Chinese-Cantonese': [0.5, 0.5348, 0.59],\n",
              " 'Turkish': [0.6, 0.3433, 0.7],\n",
              " 'Serbian': [0.8, 0.8632, 0.62],\n",
              " 'French': [0.5, 0.427, 0.46],\n",
              " 'Portuguese': [0.5, 0.5687, 0.47],\n",
              " 'Bosnian': [0.8, 0.5687, 0.61],\n",
              " 'Finnish': [0.5, 0.2736, 0.53],\n",
              " 'Albanian': [0.6, 0.5687, 0.57],\n",
              " 'Thai': [0.5, 0.39, 0.56],\n",
              " 'Indonesian': [0.5, 0.2736, 0.52],\n",
              " 'Hindi': [0.5, 0.3433, 0.59],\n",
              " 'Telugu': [0.5, 0.3498, 0.74],\n",
              " 'Vietnamese': [0.5, 0.427, 0.57],\n",
              " 'Suundi': [0.6, 0.5687, 0.57],\n",
              " 'Mongol': [0.6, 0.5687, 0.57],\n",
              " 'Farsi': [0.6, 0.5687, 0.57],\n",
              " 'Zulu': [0.6, 0.5335, 0.7],\n",
              " 'Romanian': [0.5, 0.3498, 0.53],\n",
              " 'Swahili': [0.6, 0.5687, 0.57],\n",
              " 'Hebrew': [0.6, 0.5687, 0.52],\n",
              " 'Azerbaijani': [0.6, 0.5687, 0.57],\n",
              " 'Russian, Ukrainian': [0.5, 0.5687, 0.51],\n",
              " 'Slovak': [0.6, 0.5687, 0.57],\n",
              " 'Panjabi': [0.6, 0.5687, 0.65],\n",
              " 'Igbo': [0.5, 0.427, 0.61],\n",
              " 'Slovenian': [0.5, 0.5687, 0.56],\n",
              " 'Gujarati': [0.6, 0.5687, 0.68],\n",
              " 'Tamil': [0.6, 0.5687, 0.71],\n",
              " 'Pulaar': [0.7, 0.5687, 0.57],\n",
              " 'Maldivian': [0.9, 0.5687, 0.81],\n",
              " 'Mongolian': [0.6, 0.5687, 0.57],\n",
              " 'Sinhala': [0.6, 0.4121, 0.78],\n",
              " 'Welsh': [0.8, 0.5687, 0.57],\n",
              " 'Bengali': [0.6, 0.4054, 0.7],\n",
              " 'Catalan': [0.5, 0.2804, 0.53]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "language_distance_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1MdszAJGuDb"
      },
      "source": [
        "# Putting together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuRc2mATpiPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "631eea31-2252-409a-9f7d-9bf61ddcd1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_sent_len, ttr_word, ave_word_len, function_word_type, function_word_H, function_word_std, function_word_range, lexical_density, ttr_lemma, ave_lemma_len, function_lemma_type, function_lemma_H, function_lemma_std, function_lemma_range, num_pos_type, pos_H, pos_std, pos_range, verb_word_H, verb_word_std, verb_word_range, verb_lemma_H, verb_lemma_std, verb_lemma_range, verb_mood_H, verb_mood_std, verb_mood_range, verb_number_H, verb_number_std, verb_number_range, verb_person_H, verb_person_std, verb_person_range, verb_tense_H, verb_tense_std, verb_tense_range, verb_form_H, verb_form_std, verb_form_range, verb_valency_H, verb_valency_std, verb_valency_range, verb_aspect_H, verb_aspect_std, verb_aspect_range, aux_word_H, aux_word_std, aux_word_range, aux_lemma_H, aux_lemma_std, aux_lemma_range, aux_mood_H, aux_mood_std, aux_mood_range, aux_number_H, aux_number_std, aux_number_range, aux_person_H, aux_person_std, aux_person_range, aux_tense_H, aux_tense_std, aux_tense_range, aux_form_H, aux_form_std, aux_form_range, deprel_H, deprel_std, deprel_range, subordinate_deprel_H, subordinate_deprel_std, subordinate_deprel_range, subordinate_order_H, subordinate_order_std, subordinate_order_range, head_finality, verb_valency_H, verb_valency_std, verb_valency_range, word_order_H, word_order_std, word_order_range, ave_dep_len, ave_clause_len, non_projective_sent_ratio, non_projective_word_ratio, ave_tree_depth, subordinate_non_projective_sent, subordinate_non_projective_word_ratio, subordinate_ave_tree_depth, verb_raio, s_v, v_s, v_o, o_v, subordinate_head_initial, subordinate_head_final, verb_mood_Ind, verb_mood_Imp, verb_mood_Cnd, verb_mood_Pot, verb_mood_Sub, verb_mood_Jus, verb_mood_Prp, verb_mood_Qot, verb_mood_Opt, verb_mood_Des, verb_mood_Nec, verb_mood_Irr, verb_mood_Adm, verb_number_Sing, verb_number_Plur, verb_number_Dual, verb_number_Tri, verb_number_Pau, verb_number_Grpa, verb_number_Grpl, verb_number_Inv, verb_number_Count, verb_number_Ptan, verb_number_Coll, verb_person_0, verb_person_1, verb_person_2, verb_person_3, verb_person_4, verb_tense_Past, verb_tense_Pres, verb_tense_Fut, verb_tense_Imp, verb_tense_Pqp, verb_form_Fin, verb_form_Inf, verb_form_Sup, verb_form_Part, verb_form_Conv, verb_form_Gdv, verb_form_Ger, verb_form_Vnoun, verb_aspect_Imp, verb_aspect_Perf, verb_aspect_Prosp, verb_aspect_Prog, verb_aspect_Hab, verb_aspect_Iter, verb_voice_Act, verb_voice_Mid, verb_voice_Rcp, verb_voice_Pass, verb_voice_Antip, verb_voice_Lfoc, verb_voice_Bfoc, verb_voice_Dir, verb_voice_Inv, verb_voice_Cau, aux_mood_Ind, aux_mood_Imp, aux_mood_Cnd, aux_mood_Pot, aux_mood_Sub, aux_mood_Jus, aux_mood_Prp, aux_mood_Qot, aux_mood_Opt, aux_mood_Des, aux_mood_Nec, aux_mood_Irr, aux_mood_Adm, aux_number_Sing, aux_number_Plur, aux_number_Dual, aux_number_Tri, aux_number_Pau, aux_number_Grpa, aux_number_Grpl, aux_number_Inv, aux_number_Count, aux_number_Ptan, aux_number_Coll, aux_person_0, aux_person_1, aux_person_2, aux_person_3, aux_person_4, aux_tense_Past, aux_tense_Pres, aux_tense_Fut, aux_tense_Imp, aux_tense_Pqp, aux_form_Fin, aux_form_Inf, aux_form_Sup, aux_form_Part, aux_form_Conv, aux_form_Gdv, aux_form_Ger, aux_form_Vnoun, aux_voice_Act, aux_voice_Mid, aux_voice_Rcp, aux_voice_Pass, aux_voice_Antip, aux_voice_Lfoc, aux_voice_Bfoc, aux_voice_Dir, aux_voice_Inv, aux_voice_Cau|L1|doc2vec_1, doc2vec_2, doc2vec_3, doc2vec_4, doc2vec_5, doc2vec_6, doc2vec_7, doc2vec_8, doc2vec_9, doc2vec_10, doc2vec_11, doc2vec_12, doc2vec_13, doc2vec_14, doc2vec_15, doc2vec_16, doc2vec_17, doc2vec_18, doc2vec_19, doc2vec_20, doc2vec_21, doc2vec_22, doc2vec_23, doc2vec_24, doc2vec_25, doc2vec_26, doc2vec_27, doc2vec_28, doc2vec_29, doc2vec_30, doc2vec_31, doc2vec_32, doc2vec_33, doc2vec_34, doc2vec_35, doc2vec_36, doc2vec_37, doc2vec_38, doc2vec_39, doc2vec_40, doc2vec_41, doc2vec_42, doc2vec_43, doc2vec_44, doc2vec_45, doc2vec_46, doc2vec_47, doc2vec_48, doc2vec_49, doc2vec_50, doc2vec_51, doc2vec_52, doc2vec_53, doc2vec_54, doc2vec_55, doc2vec_56, doc2vec_57, doc2vec_58, doc2vec_59, doc2vec_60, doc2vec_61, doc2vec_62, doc2vec_63, doc2vec_64, doc2vec_65, doc2vec_66, doc2vec_67, doc2vec_68, doc2vec_69, doc2vec_70, doc2vec_71, doc2vec_72, doc2vec_73, doc2vec_74, doc2vec_75, doc2vec_76, doc2vec_77, doc2vec_78, doc2vec_79, doc2vec_80, doc2vec_81, doc2vec_82, doc2vec_83, doc2vec_84, doc2vec_85, doc2vec_86, doc2vec_87, doc2vec_88, doc2vec_89, doc2vec_90, doc2vec_91, doc2vec_92, doc2vec_93, doc2vec_94, doc2vec_95, doc2vec_96, doc2vec_97, doc2vec_98, doc2vec_99, doc2vec_100, doc2vec_101, doc2vec_102, doc2vec_103, doc2vec_104, doc2vec_105, doc2vec_106, doc2vec_107, doc2vec_108, doc2vec_109, doc2vec_110, doc2vec_111, doc2vec_112, doc2vec_113, doc2vec_114, doc2vec_115, doc2vec_116, doc2vec_117, doc2vec_118, doc2vec_119, doc2vec_120, doc2vec_121, doc2vec_122, doc2vec_123, doc2vec_124, doc2vec_125, doc2vec_126, doc2vec_127, doc2vec_128, doc2vec_129, doc2vec_130, doc2vec_131, doc2vec_132, doc2vec_133, doc2vec_134, doc2vec_135, doc2vec_136, doc2vec_137, doc2vec_138, doc2vec_139, doc2vec_140, doc2vec_141, doc2vec_142, doc2vec_143, doc2vec_144, doc2vec_145, doc2vec_146, doc2vec_147, doc2vec_148, doc2vec_149, doc2vec_150, doc2vec_151, doc2vec_152, doc2vec_153, doc2vec_154, doc2vec_155, doc2vec_156, doc2vec_157, doc2vec_158, doc2vec_159, doc2vec_160, doc2vec_161, doc2vec_162, doc2vec_163, doc2vec_164, doc2vec_165, doc2vec_166, doc2vec_167, doc2vec_168, doc2vec_169, doc2vec_170, doc2vec_171, doc2vec_172, doc2vec_173, doc2vec_174, doc2vec_175, doc2vec_176, doc2vec_177, doc2vec_178, doc2vec_179, doc2vec_180, doc2vec_181, doc2vec_182, doc2vec_183, doc2vec_184, doc2vec_185, doc2vec_186, doc2vec_187, doc2vec_188, doc2vec_189, doc2vec_190, doc2vec_191, doc2vec_192, doc2vec_193, doc2vec_194, doc2vec_195, doc2vec_196, doc2vec_197, doc2vec_198, doc2vec_199, doc2vec_200, doc2vec_201, doc2vec_202, doc2vec_203, doc2vec_204, doc2vec_205, doc2vec_206, doc2vec_207, doc2vec_208, doc2vec_209, doc2vec_210, doc2vec_211, doc2vec_212, doc2vec_213, doc2vec_214, doc2vec_215, doc2vec_216, doc2vec_217, doc2vec_218, doc2vec_219, doc2vec_220, doc2vec_221, doc2vec_222, doc2vec_223, doc2vec_224, doc2vec_225, doc2vec_226, doc2vec_227, doc2vec_228, doc2vec_229, doc2vec_230, doc2vec_231, doc2vec_232, doc2vec_233, doc2vec_234, doc2vec_235, doc2vec_236, doc2vec_237, doc2vec_238, doc2vec_239, doc2vec_240, doc2vec_241, doc2vec_242, doc2vec_243, doc2vec_244, doc2vec_245, doc2vec_246, doc2vec_247, doc2vec_248, doc2vec_249, doc2vec_250, doc2vec_251, doc2vec_252, doc2vec_253, doc2vec_254, doc2vec_255, doc2vec_256, doc2vec_257, doc2vec_258, doc2vec_259, doc2vec_260, doc2vec_261, doc2vec_262, doc2vec_263, doc2vec_264, doc2vec_265, doc2vec_266, doc2vec_267, doc2vec_268, doc2vec_269, doc2vec_270, doc2vec_271, doc2vec_272, doc2vec_273, doc2vec_274, doc2vec_275, doc2vec_276, doc2vec_277, doc2vec_278, doc2vec_279, doc2vec_280, doc2vec_281, doc2vec_282, doc2vec_283, doc2vec_284, doc2vec_285, doc2vec_286, doc2vec_287, doc2vec_288, doc2vec_289, doc2vec_290, doc2vec_291, doc2vec_292, doc2vec_293, doc2vec_294, doc2vec_295, doc2vec_296, doc2vec_297, doc2vec_298, doc2vec_299, doc2vec_300|featural, phonological, syntactic\n",
            "\n"
          ]
        }
      ],
      "source": [
        "morph_header = [\n",
        " 'ave_sent_len', 'ttr_word', 'ave_word_len',\n",
        " 'function_word_type', 'function_word_H', 'function_word_std', 'function_word_range',\n",
        " 'lexical_density', 'ttr_lemma', 'ave_lemma_len',\n",
        " 'function_lemma_type', 'function_lemma_H', 'function_lemma_std', 'function_lemma_range',\n",
        " 'num_pos_type', 'pos_H', 'pos_std', 'pos_range',\n",
        " 'verb_word_H', 'verb_word_std', 'verb_word_range',\n",
        " 'verb_lemma_H', 'verb_lemma_std', 'verb_lemma_range',\n",
        " 'verb_mood_H', 'verb_mood_std', 'verb_mood_range',\n",
        " 'verb_number_H', 'verb_number_std', 'verb_number_range',\n",
        " 'verb_person_H', 'verb_person_std', 'verb_person_range',\n",
        " 'verb_tense_H', 'verb_tense_std', 'verb_tense_range',\n",
        " 'verb_form_H', 'verb_form_std', 'verb_form_range',\n",
        " 'verb_valency_H', 'verb_valency_std', 'verb_valency_range',\n",
        " 'verb_aspect_H', 'verb_aspect_std', 'verb_aspect_range',\n",
        " 'aux_word_H', 'aux_word_std', 'aux_word_range',\n",
        " 'aux_lemma_H', 'aux_lemma_std', 'aux_lemma_range',\n",
        " 'aux_mood_H', 'aux_mood_std', 'aux_mood_range',\n",
        " 'aux_number_H', 'aux_number_std', 'aux_number_range',\n",
        " 'aux_person_H', 'aux_person_std', 'aux_person_range',\n",
        " 'aux_tense_H', 'aux_tense_std', 'aux_tense_range',\n",
        " 'aux_form_H', 'aux_form_std', 'aux_form_range',\n",
        " 'deprel_H' , 'deprel_std', 'deprel_range',\n",
        " 'subordinate_deprel_H', 'subordinate_deprel_std', 'subordinate_deprel_range',\n",
        " 'subordinate_order_H', 'subordinate_order_std', 'subordinate_order_range','head_finality',\n",
        " 'verb_valency_H', 'verb_valency_std', 'verb_valency_range',\n",
        " 'word_order_H', 'word_order_std', 'word_order_range',\n",
        " 'ave_dep_len', 'ave_clause_len',\n",
        " 'non_projective_sent_ratio', 'non_projective_word_ratio', 'ave_tree_depth',\n",
        " 'subordinate_non_projective_sent', 'subordinate_non_projective_word_ratio',\n",
        " 'subordinate_ave_tree_depth', 'verb_raio',\n",
        " 's_v', 'v_s', 'v_o', 'o_v',\n",
        " 'subordinate_head_initial', 'subordinate_head_final',\n",
        " 'verb_mood_Ind', 'verb_mood_Imp', 'verb_mood_Cnd', 'verb_mood_Pot', 'verb_mood_Sub',\n",
        " 'verb_mood_Jus',  'verb_mood_Prp', 'verb_mood_Qot', 'verb_mood_Opt', 'verb_mood_Des',\n",
        " 'verb_mood_Nec', 'verb_mood_Irr', 'verb_mood_Adm',\n",
        " 'verb_number_Sing', 'verb_number_Plur', 'verb_number_Dual', 'verb_number_Tri',\n",
        " 'verb_number_Pau', 'verb_number_Grpa', 'verb_number_Grpl', 'verb_number_Inv',\n",
        " 'verb_number_Count', 'verb_number_Ptan', 'verb_number_Coll',\n",
        " 'verb_person_0', 'verb_person_1', 'verb_person_2', 'verb_person_3', 'verb_person_4',\n",
        " 'verb_tense_Past', 'verb_tense_Pres', 'verb_tense_Fut', 'verb_tense_Imp', 'verb_tense_Pqp',\n",
        " 'verb_form_Fin', 'verb_form_Inf', 'verb_form_Sup', 'verb_form_Part', 'verb_form_Conv',\n",
        " 'verb_form_Gdv', 'verb_form_Ger', 'verb_form_Vnoun',\n",
        " 'verb_aspect_Imp', 'verb_aspect_Perf', 'verb_aspect_Prosp', 'verb_aspect_Prog',\n",
        " 'verb_aspect_Hab', 'verb_aspect_Iter',\n",
        " 'verb_voice_Act', 'verb_voice_Mid', 'verb_voice_Rcp', 'verb_voice_Pass', 'verb_voice_Antip',\n",
        " 'verb_voice_Lfoc', 'verb_voice_Bfoc', 'verb_voice_Dir', 'verb_voice_Inv', 'verb_voice_Cau',\n",
        " 'aux_mood_Ind', 'aux_mood_Imp', 'aux_mood_Cnd', 'aux_mood_Pot', 'aux_mood_Sub',\n",
        " 'aux_mood_Jus', 'aux_mood_Prp', 'aux_mood_Qot', 'aux_mood_Opt', 'aux_mood_Des',\n",
        " 'aux_mood_Nec', 'aux_mood_Irr', 'aux_mood_Adm',\n",
        " 'aux_number_Sing', 'aux_number_Plur', 'aux_number_Dual', 'aux_number_Tri', 'aux_number_Pau',\n",
        " 'aux_number_Grpa', 'aux_number_Grpl', 'aux_number_Inv', 'aux_number_Count', 'aux_number_Ptan',\n",
        " 'aux_number_Coll',\n",
        " 'aux_person_0', 'aux_person_1', 'aux_person_2', 'aux_person_3', 'aux_person_4',\n",
        " 'aux_tense_Past', 'aux_tense_Pres', 'aux_tense_Fut', 'aux_tense_Imp', 'aux_tense_Pqp',\n",
        " 'aux_form_Fin', 'aux_form_Inf', 'aux_form_Sup', 'aux_form_Part', 'aux_form_Conv',\n",
        " 'aux_form_Gdv', 'aux_form_Ger', 'aux_form_Vnoun',\n",
        " 'aux_voice_Act', 'aux_voice_Mid', 'aux_voice_Rcp', 'aux_voice_Pass', 'aux_voice_Antip',\n",
        " 'aux_voice_Lfoc', 'aux_voice_Bfoc', 'aux_voice_Dir', 'aux_voice_Inv', 'aux_voice_Cau']\n",
        "\n",
        "doc2vec_header = []\n",
        "for i in range(1,301):\n",
        "    doc2vec_header.append(f'doc2vec_{i}')\n",
        "\n",
        "lang2vec_header = 'featural, phonological, syntactic'\n",
        "header = ', '.join(morph_header) + '|L1|' + ', '.join(doc2vec_header) + '|' + lang2vec_header + '\\n'\n",
        "print(header)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_yjIIx8UjAH"
      },
      "outputs": [],
      "source": [
        "# unzip input files\n",
        "input_extraction_path = f'/content/unzipped_inputs/{corpus}'\n",
        "shutil.unpack_archive(input_directory, input_extraction_path, 'zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTM__QGsWk7h",
        "outputId": "bb5c4bd9-c9df-4b80-de93-2545c4391f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 167/25072 [02:14<5:33:37,  1.24it/s]"
          ]
        }
      ],
      "source": [
        "file_list = [filename for filename in os.listdir(input_extraction_path) if filename.endswith('.pred')]\n",
        "file_list.sort()\n",
        "\n",
        "header_temp = header.split(', ')\n",
        "\n",
        "with open(output_directory, 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(header)\n",
        "    for file_name in tqdm(file_list):\n",
        "        if file_name.endswith('.pred'):\n",
        "            lg, all_info = morphological_feature_analysis(os.path.join(input_extraction_path, file_name))\n",
        "            if lg not in first_language_filtered and all_info != None :\n",
        "                if len(all_info) != len(header.split('|')[0].split(', ')):\n",
        "                    raise ValueError ('feature lenth not compatible')\n",
        "                if nativeness == 'Native':\n",
        "                    lang2vec = [0, 0, 0]\n",
        "                else:\n",
        "                    try:\n",
        "                        lang2vec = language_distance_scores[lg]\n",
        "                    except KeyError:\n",
        "                        print(f'essay having {lg} as its first language was skipped')\n",
        "\n",
        "                doc2vec = extract_topic_embedding(file_name)\n",
        "                # write morphological features\n",
        "                outfile.write(', '.join(str(w) for w in all_info) + '|')\n",
        "                # write first language\n",
        "                outfile.write(lg + '|')\n",
        "                # write doc2vec embedding\n",
        "                outfile.write(', '.join(str(w) for w in doc2vec) + '|')\n",
        "                # write lang2vec distance scores\n",
        "                outfile.write(', '.join(str(w) for w in lang2vec) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvgTObn_zq-o"
      },
      "outputs": [],
      "source": [
        "# Call the function to clear the /content/ directory after using the unzipped files\n",
        "def clear_content_directory():\n",
        "    content_dir = '/content/'\n",
        "    exempt_folders = ['drive', 'sample_data']\n",
        "\n",
        "    for item in os.listdir(content_dir):\n",
        "        item_path = os.path.join(content_dir, item)\n",
        "        if os.path.isdir(item_path) and item not in exempt_folders:\n",
        "            for root, dirs, files in os.walk(item_path, topdown=False):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    os.remove(file_path)\n",
        "                for dir in dirs:\n",
        "                    dir_path = os.path.join(root, dir)\n",
        "                    os.rmdir(dir_path)\n",
        "            os.rmdir(item_path)\n",
        "        elif os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "\n",
        "# clear temporary /content/ dir\n",
        "clear_content_directory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX3DSZX21_bi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ekaGZ9-vJksu",
        "8nR0wq4DGnzk",
        "Bxghr09RgSNE"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}